[2023-07-22 21:00:22,443] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:01:43,782] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 7662 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:01:43,783] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 7566 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:01:43,783] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8938 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:01:43,783] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 7575 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:01:43,783] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 7998 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:01:43,783] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9631 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:01:43,783] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8753 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:01:43,783] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9172 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:01:43,783] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8732 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:01:43,783] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8529 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:01:43,783] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9555 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:01:43,783] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8712 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:01:50,351] INFO [BrokerToControllerChannelManager broker=0 name=forwarding] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:03:00,916] INFO Creating topic _confluent-ksql-default__command_topic with configuration {cleanup.policy=delete, min.insync.replicas=1, retention.ms=-1, unclean.leader.election.enable=false} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-22 21:03:00,952] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-ksql-default__command_topic-0) (kafka.server.ReplicaFetcherManager)
[2023-07-22 21:03:00,957] INFO [MergedLog partition=_confluent-ksql-default__command_topic-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:00,959] INFO Created log for partition _confluent-ksql-default__command_topic-0 in /tmp/kafka-logs/_confluent-ksql-default__command_topic-0 with properties {cleanup.policy=delete, min.insync.replicas=1, retention.ms=-1, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:00,961] INFO [Partition _confluent-ksql-default__command_topic-0 broker=0] No checkpointed highwatermark is found for partition _confluent-ksql-default__command_topic-0 (kafka.cluster.Partition)
[2023-07-22 21:03:00,961] INFO [Partition _confluent-ksql-default__command_topic-0 broker=0] Log loaded for partition _confluent-ksql-default__command_topic-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:00,962] INFO Setting topicIdPartition 6-BOKbesRqKS-zpDvQAedg:_confluent-ksql-default__command_topic-0 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:00,962] INFO [MergedLog partition=_confluent-ksql-default__command_topic-0, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for _confluent-ksql-default__command_topic-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:00,974] INFO Creating topic default_ksql_processing_log with configuration {} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-22 21:03:00,997] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(default_ksql_processing_log-0) (kafka.server.ReplicaFetcherManager)
[2023-07-22 21:03:01,001] INFO [MergedLog partition=default_ksql_processing_log-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,002] INFO Created log for partition default_ksql_processing_log-0 in /tmp/kafka-logs/default_ksql_processing_log-0 with properties {} (kafka.log.LogManager)
[2023-07-22 21:03:01,004] INFO [Partition default_ksql_processing_log-0 broker=0] No checkpointed highwatermark is found for partition default_ksql_processing_log-0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,004] INFO [Partition default_ksql_processing_log-0 broker=0] Log loaded for partition default_ksql_processing_log-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,004] INFO Setting topicIdPartition ukxUUqnuRoKw7QSyAX3q1g:default_ksql_processing_log-0 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,005] INFO [MergedLog partition=default_ksql_processing_log-0, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for default_ksql_processing_log-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,586] INFO Creating topic __transaction_state with configuration {compression.type=uncompressed, cleanup.policy=compact, min.insync.replicas=1, segment.bytes=104857600, confluent.placement.constraints=, unclean.leader.election.enable=false} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-22 21:03:01,713] INFO Creating topic __transaction_state with configuration {compression.type=uncompressed, cleanup.policy=compact, min.insync.replicas=1, segment.bytes=104857600, confluent.placement.constraints=, unclean.leader.election.enable=false} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-22 21:03:01,834] INFO Creating topic __transaction_state with configuration {compression.type=uncompressed, cleanup.policy=compact, min.insync.replicas=1, segment.bytes=104857600, confluent.placement.constraints=, unclean.leader.election.enable=false} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-22 21:03:01,892] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__transaction_state-42, __transaction_state-7, __transaction_state-13, __transaction_state-0, __transaction_state-37, __transaction_state-6, __transaction_state-32, __transaction_state-18, __transaction_state-40, __transaction_state-31, __transaction_state-45, __transaction_state-15, __transaction_state-12, __transaction_state-46, __transaction_state-48, __transaction_state-49, __transaction_state-28, __transaction_state-2, __transaction_state-20, __transaction_state-24, __transaction_state-3, __transaction_state-21, __transaction_state-29, __transaction_state-39, __transaction_state-38, __transaction_state-14, __transaction_state-10, __transaction_state-44, __transaction_state-9, __transaction_state-22, __transaction_state-43, __transaction_state-4, __transaction_state-30, __transaction_state-33, __transaction_state-25, __transaction_state-17, __transaction_state-23, __transaction_state-47, __transaction_state-26, __transaction_state-36, __transaction_state-5, __transaction_state-8, __transaction_state-16, __transaction_state-11, __transaction_state-19, __transaction_state-27, __transaction_state-41, __transaction_state-1, __transaction_state-34, __transaction_state-35) (kafka.server.ReplicaFetcherManager)
[2023-07-22 21:03:01,898] INFO [MergedLog partition=__transaction_state-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,901] INFO Created log for partition __transaction_state-3 in /tmp/kafka-logs/__transaction_state-3 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,903] INFO [Partition __transaction_state-3 broker=0] No checkpointed highwatermark is found for partition __transaction_state-3 (kafka.cluster.Partition)
[2023-07-22 21:03:01,903] INFO [Partition __transaction_state-3 broker=0] Log loaded for partition __transaction_state-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,903] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-3 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,903] INFO [MergedLog partition=__transaction_state-3, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,908] INFO [MergedLog partition=__transaction_state-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,910] INFO Created log for partition __transaction_state-18 in /tmp/kafka-logs/__transaction_state-18 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,911] INFO [Partition __transaction_state-18 broker=0] No checkpointed highwatermark is found for partition __transaction_state-18 (kafka.cluster.Partition)
[2023-07-22 21:03:01,911] INFO [Partition __transaction_state-18 broker=0] Log loaded for partition __transaction_state-18 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,911] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-18 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,912] INFO [MergedLog partition=__transaction_state-18, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,916] INFO [MergedLog partition=__transaction_state-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,917] INFO Created log for partition __transaction_state-33 in /tmp/kafka-logs/__transaction_state-33 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,917] INFO [Partition __transaction_state-33 broker=0] No checkpointed highwatermark is found for partition __transaction_state-33 (kafka.cluster.Partition)
[2023-07-22 21:03:01,917] INFO [Partition __transaction_state-33 broker=0] Log loaded for partition __transaction_state-33 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,918] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-33 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,918] INFO [MergedLog partition=__transaction_state-33, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,922] INFO [MergedLog partition=__transaction_state-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,923] INFO Created log for partition __transaction_state-11 in /tmp/kafka-logs/__transaction_state-11 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,924] INFO [Partition __transaction_state-11 broker=0] No checkpointed highwatermark is found for partition __transaction_state-11 (kafka.cluster.Partition)
[2023-07-22 21:03:01,924] INFO [Partition __transaction_state-11 broker=0] Log loaded for partition __transaction_state-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,924] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-11 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,925] INFO [MergedLog partition=__transaction_state-11, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,929] INFO [MergedLog partition=__transaction_state-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,930] INFO Created log for partition __transaction_state-26 in /tmp/kafka-logs/__transaction_state-26 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,931] INFO [Partition __transaction_state-26 broker=0] No checkpointed highwatermark is found for partition __transaction_state-26 (kafka.cluster.Partition)
[2023-07-22 21:03:01,931] INFO [Partition __transaction_state-26 broker=0] Log loaded for partition __transaction_state-26 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,931] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-26 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,932] INFO [MergedLog partition=__transaction_state-26, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,937] INFO [MergedLog partition=__transaction_state-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,938] INFO Created log for partition __transaction_state-41 in /tmp/kafka-logs/__transaction_state-41 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,939] INFO [Partition __transaction_state-41 broker=0] No checkpointed highwatermark is found for partition __transaction_state-41 (kafka.cluster.Partition)
[2023-07-22 21:03:01,939] INFO [Partition __transaction_state-41 broker=0] Log loaded for partition __transaction_state-41 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,939] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-41 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,939] INFO [MergedLog partition=__transaction_state-41, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,943] INFO [MergedLog partition=__transaction_state-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,946] INFO Created log for partition __transaction_state-4 in /tmp/kafka-logs/__transaction_state-4 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,946] INFO [Partition __transaction_state-4 broker=0] No checkpointed highwatermark is found for partition __transaction_state-4 (kafka.cluster.Partition)
[2023-07-22 21:03:01,946] INFO [Partition __transaction_state-4 broker=0] Log loaded for partition __transaction_state-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,947] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-4 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,947] INFO [MergedLog partition=__transaction_state-4, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,948] INFO Creating topic __transaction_state with configuration {compression.type=uncompressed, cleanup.policy=compact, min.insync.replicas=1, segment.bytes=104857600, confluent.placement.constraints=, unclean.leader.election.enable=false} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-22 21:03:01,951] INFO [MergedLog partition=__transaction_state-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,953] INFO Created log for partition __transaction_state-19 in /tmp/kafka-logs/__transaction_state-19 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,953] INFO [Partition __transaction_state-19 broker=0] No checkpointed highwatermark is found for partition __transaction_state-19 (kafka.cluster.Partition)
[2023-07-22 21:03:01,953] INFO [Partition __transaction_state-19 broker=0] Log loaded for partition __transaction_state-19 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,953] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-19 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,953] INFO [MergedLog partition=__transaction_state-19, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,958] INFO [MergedLog partition=__transaction_state-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,960] INFO Created log for partition __transaction_state-34 in /tmp/kafka-logs/__transaction_state-34 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,961] INFO [Partition __transaction_state-34 broker=0] No checkpointed highwatermark is found for partition __transaction_state-34 (kafka.cluster.Partition)
[2023-07-22 21:03:01,961] INFO [Partition __transaction_state-34 broker=0] Log loaded for partition __transaction_state-34 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,961] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-34 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,961] INFO [MergedLog partition=__transaction_state-34, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,965] INFO [MergedLog partition=__transaction_state-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,968] INFO Created log for partition __transaction_state-49 in /tmp/kafka-logs/__transaction_state-49 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,968] INFO [Partition __transaction_state-49 broker=0] No checkpointed highwatermark is found for partition __transaction_state-49 (kafka.cluster.Partition)
[2023-07-22 21:03:01,968] INFO [Partition __transaction_state-49 broker=0] Log loaded for partition __transaction_state-49 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,968] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-49 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,969] INFO [MergedLog partition=__transaction_state-49, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,971] INFO [MergedLog partition=__transaction_state-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,973] INFO Created log for partition __transaction_state-12 in /tmp/kafka-logs/__transaction_state-12 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,973] INFO [Partition __transaction_state-12 broker=0] No checkpointed highwatermark is found for partition __transaction_state-12 (kafka.cluster.Partition)
[2023-07-22 21:03:01,973] INFO [Partition __transaction_state-12 broker=0] Log loaded for partition __transaction_state-12 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,974] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-12 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,974] INFO [MergedLog partition=__transaction_state-12, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,977] INFO [MergedLog partition=__transaction_state-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,978] INFO Created log for partition __transaction_state-27 in /tmp/kafka-logs/__transaction_state-27 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,978] INFO [Partition __transaction_state-27 broker=0] No checkpointed highwatermark is found for partition __transaction_state-27 (kafka.cluster.Partition)
[2023-07-22 21:03:01,978] INFO [Partition __transaction_state-27 broker=0] Log loaded for partition __transaction_state-27 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,979] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-27 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,979] INFO [MergedLog partition=__transaction_state-27, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,982] INFO [MergedLog partition=__transaction_state-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,983] INFO Created log for partition __transaction_state-42 in /tmp/kafka-logs/__transaction_state-42 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,983] INFO [Partition __transaction_state-42 broker=0] No checkpointed highwatermark is found for partition __transaction_state-42 (kafka.cluster.Partition)
[2023-07-22 21:03:01,983] INFO [Partition __transaction_state-42 broker=0] Log loaded for partition __transaction_state-42 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,983] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-42 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,983] INFO [MergedLog partition=__transaction_state-42, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,987] INFO [MergedLog partition=__transaction_state-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,988] INFO Created log for partition __transaction_state-24 in /tmp/kafka-logs/__transaction_state-24 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,988] INFO [Partition __transaction_state-24 broker=0] No checkpointed highwatermark is found for partition __transaction_state-24 (kafka.cluster.Partition)
[2023-07-22 21:03:01,988] INFO [Partition __transaction_state-24 broker=0] Log loaded for partition __transaction_state-24 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,989] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-24 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,989] INFO [MergedLog partition=__transaction_state-24, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,992] INFO [MergedLog partition=__transaction_state-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,993] INFO Created log for partition __transaction_state-39 in /tmp/kafka-logs/__transaction_state-39 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:01,994] INFO [Partition __transaction_state-39 broker=0] No checkpointed highwatermark is found for partition __transaction_state-39 (kafka.cluster.Partition)
[2023-07-22 21:03:01,994] INFO [Partition __transaction_state-39 broker=0] Log loaded for partition __transaction_state-39 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:01,994] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-39 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:01,995] INFO [MergedLog partition=__transaction_state-39, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:01,998] INFO [MergedLog partition=__transaction_state-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:01,999] INFO Created log for partition __transaction_state-1 in /tmp/kafka-logs/__transaction_state-1 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,000] INFO [Partition __transaction_state-1 broker=0] No checkpointed highwatermark is found for partition __transaction_state-1 (kafka.cluster.Partition)
[2023-07-22 21:03:02,000] INFO [Partition __transaction_state-1 broker=0] Log loaded for partition __transaction_state-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,000] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-1 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,000] INFO [MergedLog partition=__transaction_state-1, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,004] INFO [MergedLog partition=__transaction_state-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,006] INFO Created log for partition __transaction_state-32 in /tmp/kafka-logs/__transaction_state-32 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,006] INFO [Partition __transaction_state-32 broker=0] No checkpointed highwatermark is found for partition __transaction_state-32 (kafka.cluster.Partition)
[2023-07-22 21:03:02,007] INFO [Partition __transaction_state-32 broker=0] Log loaded for partition __transaction_state-32 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,007] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-32 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,007] INFO [MergedLog partition=__transaction_state-32, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,011] INFO [MergedLog partition=__transaction_state-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,012] INFO Created log for partition __transaction_state-47 in /tmp/kafka-logs/__transaction_state-47 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,012] INFO [Partition __transaction_state-47 broker=0] No checkpointed highwatermark is found for partition __transaction_state-47 (kafka.cluster.Partition)
[2023-07-22 21:03:02,012] INFO [Partition __transaction_state-47 broker=0] Log loaded for partition __transaction_state-47 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,012] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-47 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,013] INFO [MergedLog partition=__transaction_state-47, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,017] INFO [MergedLog partition=__transaction_state-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,019] INFO Created log for partition __transaction_state-9 in /tmp/kafka-logs/__transaction_state-9 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,019] INFO [Partition __transaction_state-9 broker=0] No checkpointed highwatermark is found for partition __transaction_state-9 (kafka.cluster.Partition)
[2023-07-22 21:03:02,019] INFO [Partition __transaction_state-9 broker=0] Log loaded for partition __transaction_state-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,019] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-9 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,020] INFO [MergedLog partition=__transaction_state-9, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,023] INFO [MergedLog partition=__transaction_state-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,024] INFO Created log for partition __transaction_state-40 in /tmp/kafka-logs/__transaction_state-40 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,024] INFO [Partition __transaction_state-40 broker=0] No checkpointed highwatermark is found for partition __transaction_state-40 (kafka.cluster.Partition)
[2023-07-22 21:03:02,024] INFO [Partition __transaction_state-40 broker=0] Log loaded for partition __transaction_state-40 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,024] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-40 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,025] INFO [MergedLog partition=__transaction_state-40, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,028] INFO [MergedLog partition=__transaction_state-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,029] INFO Created log for partition __transaction_state-2 in /tmp/kafka-logs/__transaction_state-2 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,029] INFO [Partition __transaction_state-2 broker=0] No checkpointed highwatermark is found for partition __transaction_state-2 (kafka.cluster.Partition)
[2023-07-22 21:03:02,029] INFO [Partition __transaction_state-2 broker=0] Log loaded for partition __transaction_state-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,029] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-2 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,031] INFO [MergedLog partition=__transaction_state-2, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,033] INFO [MergedLog partition=__transaction_state-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,035] INFO Created log for partition __transaction_state-17 in /tmp/kafka-logs/__transaction_state-17 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,035] INFO [Partition __transaction_state-17 broker=0] No checkpointed highwatermark is found for partition __transaction_state-17 (kafka.cluster.Partition)
[2023-07-22 21:03:02,035] INFO [Partition __transaction_state-17 broker=0] Log loaded for partition __transaction_state-17 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,035] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-17 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,035] INFO [MergedLog partition=__transaction_state-17, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,039] INFO [MergedLog partition=__transaction_state-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,040] INFO Created log for partition __transaction_state-48 in /tmp/kafka-logs/__transaction_state-48 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,040] INFO [Partition __transaction_state-48 broker=0] No checkpointed highwatermark is found for partition __transaction_state-48 (kafka.cluster.Partition)
[2023-07-22 21:03:02,040] INFO [Partition __transaction_state-48 broker=0] Log loaded for partition __transaction_state-48 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,040] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-48 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,041] INFO [MergedLog partition=__transaction_state-48, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,045] INFO [MergedLog partition=__transaction_state-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,046] INFO Created log for partition __transaction_state-10 in /tmp/kafka-logs/__transaction_state-10 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,046] INFO [Partition __transaction_state-10 broker=0] No checkpointed highwatermark is found for partition __transaction_state-10 (kafka.cluster.Partition)
[2023-07-22 21:03:02,046] INFO [Partition __transaction_state-10 broker=0] Log loaded for partition __transaction_state-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,048] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-10 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,048] INFO [MergedLog partition=__transaction_state-10, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,050] INFO [MergedLog partition=__transaction_state-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,052] INFO Created log for partition __transaction_state-25 in /tmp/kafka-logs/__transaction_state-25 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,052] INFO [Partition __transaction_state-25 broker=0] No checkpointed highwatermark is found for partition __transaction_state-25 (kafka.cluster.Partition)
[2023-07-22 21:03:02,052] INFO [Partition __transaction_state-25 broker=0] Log loaded for partition __transaction_state-25 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,052] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-25 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,053] INFO [MergedLog partition=__transaction_state-25, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,056] INFO [MergedLog partition=__transaction_state-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,057] INFO Created log for partition __transaction_state-7 in /tmp/kafka-logs/__transaction_state-7 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,057] INFO [Partition __transaction_state-7 broker=0] No checkpointed highwatermark is found for partition __transaction_state-7 (kafka.cluster.Partition)
[2023-07-22 21:03:02,057] INFO [Partition __transaction_state-7 broker=0] Log loaded for partition __transaction_state-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,058] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-7 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,058] INFO [MergedLog partition=__transaction_state-7, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,062] INFO Creating topic __transaction_state with configuration {compression.type=uncompressed, cleanup.policy=compact, min.insync.replicas=1, segment.bytes=104857600, confluent.placement.constraints=, unclean.leader.election.enable=false} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-22 21:03:02,063] INFO [MergedLog partition=__transaction_state-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,065] INFO Created log for partition __transaction_state-22 in /tmp/kafka-logs/__transaction_state-22 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,065] INFO [Partition __transaction_state-22 broker=0] No checkpointed highwatermark is found for partition __transaction_state-22 (kafka.cluster.Partition)
[2023-07-22 21:03:02,066] INFO [Partition __transaction_state-22 broker=0] Log loaded for partition __transaction_state-22 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,066] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-22 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,066] INFO [MergedLog partition=__transaction_state-22, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,072] INFO [MergedLog partition=__transaction_state-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,075] INFO Created log for partition __transaction_state-37 in /tmp/kafka-logs/__transaction_state-37 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,076] INFO [Partition __transaction_state-37 broker=0] No checkpointed highwatermark is found for partition __transaction_state-37 (kafka.cluster.Partition)
[2023-07-22 21:03:02,076] INFO [Partition __transaction_state-37 broker=0] Log loaded for partition __transaction_state-37 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,076] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-37 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,076] INFO [MergedLog partition=__transaction_state-37, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,080] INFO [MergedLog partition=__transaction_state-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,082] INFO Created log for partition __transaction_state-0 in /tmp/kafka-logs/__transaction_state-0 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,082] INFO [Partition __transaction_state-0 broker=0] No checkpointed highwatermark is found for partition __transaction_state-0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,082] INFO [Partition __transaction_state-0 broker=0] Log loaded for partition __transaction_state-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,082] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-0 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,082] INFO [MergedLog partition=__transaction_state-0, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,086] INFO [MergedLog partition=__transaction_state-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,088] INFO Created log for partition __transaction_state-15 in /tmp/kafka-logs/__transaction_state-15 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,088] INFO [Partition __transaction_state-15 broker=0] No checkpointed highwatermark is found for partition __transaction_state-15 (kafka.cluster.Partition)
[2023-07-22 21:03:02,088] INFO [Partition __transaction_state-15 broker=0] Log loaded for partition __transaction_state-15 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,088] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-15 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,088] INFO [MergedLog partition=__transaction_state-15, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,094] INFO [MergedLog partition=__transaction_state-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,095] INFO Created log for partition __transaction_state-30 in /tmp/kafka-logs/__transaction_state-30 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,096] INFO [Partition __transaction_state-30 broker=0] No checkpointed highwatermark is found for partition __transaction_state-30 (kafka.cluster.Partition)
[2023-07-22 21:03:02,097] INFO [Partition __transaction_state-30 broker=0] Log loaded for partition __transaction_state-30 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,097] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-30 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,097] INFO [MergedLog partition=__transaction_state-30, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,101] INFO [MergedLog partition=__transaction_state-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,103] INFO Created log for partition __transaction_state-45 in /tmp/kafka-logs/__transaction_state-45 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,103] INFO [Partition __transaction_state-45 broker=0] No checkpointed highwatermark is found for partition __transaction_state-45 (kafka.cluster.Partition)
[2023-07-22 21:03:02,103] INFO [Partition __transaction_state-45 broker=0] Log loaded for partition __transaction_state-45 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,104] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-45 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,104] INFO [MergedLog partition=__transaction_state-45, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,108] INFO [MergedLog partition=__transaction_state-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,109] INFO Created log for partition __transaction_state-8 in /tmp/kafka-logs/__transaction_state-8 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,110] INFO [Partition __transaction_state-8 broker=0] No checkpointed highwatermark is found for partition __transaction_state-8 (kafka.cluster.Partition)
[2023-07-22 21:03:02,110] INFO [Partition __transaction_state-8 broker=0] Log loaded for partition __transaction_state-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,110] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-8 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,110] INFO [MergedLog partition=__transaction_state-8, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,113] INFO [MergedLog partition=__transaction_state-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,115] INFO Created log for partition __transaction_state-23 in /tmp/kafka-logs/__transaction_state-23 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,115] INFO [Partition __transaction_state-23 broker=0] No checkpointed highwatermark is found for partition __transaction_state-23 (kafka.cluster.Partition)
[2023-07-22 21:03:02,115] INFO [Partition __transaction_state-23 broker=0] Log loaded for partition __transaction_state-23 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,115] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-23 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,115] INFO [MergedLog partition=__transaction_state-23, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,119] INFO [MergedLog partition=__transaction_state-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,120] INFO Created log for partition __transaction_state-38 in /tmp/kafka-logs/__transaction_state-38 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,120] INFO [Partition __transaction_state-38 broker=0] No checkpointed highwatermark is found for partition __transaction_state-38 (kafka.cluster.Partition)
[2023-07-22 21:03:02,120] INFO [Partition __transaction_state-38 broker=0] Log loaded for partition __transaction_state-38 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,120] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-38 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,120] INFO [MergedLog partition=__transaction_state-38, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,124] INFO [MergedLog partition=__transaction_state-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,125] INFO Created log for partition __transaction_state-16 in /tmp/kafka-logs/__transaction_state-16 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,125] INFO [Partition __transaction_state-16 broker=0] No checkpointed highwatermark is found for partition __transaction_state-16 (kafka.cluster.Partition)
[2023-07-22 21:03:02,125] INFO [Partition __transaction_state-16 broker=0] Log loaded for partition __transaction_state-16 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,125] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-16 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,125] INFO [MergedLog partition=__transaction_state-16, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,130] INFO [MergedLog partition=__transaction_state-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,131] INFO Created log for partition __transaction_state-31 in /tmp/kafka-logs/__transaction_state-31 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,131] INFO [Partition __transaction_state-31 broker=0] No checkpointed highwatermark is found for partition __transaction_state-31 (kafka.cluster.Partition)
[2023-07-22 21:03:02,131] INFO [Partition __transaction_state-31 broker=0] Log loaded for partition __transaction_state-31 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,131] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-31 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,132] INFO [MergedLog partition=__transaction_state-31, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,135] INFO [MergedLog partition=__transaction_state-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,137] INFO Created log for partition __transaction_state-46 in /tmp/kafka-logs/__transaction_state-46 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,138] INFO [Partition __transaction_state-46 broker=0] No checkpointed highwatermark is found for partition __transaction_state-46 (kafka.cluster.Partition)
[2023-07-22 21:03:02,138] INFO [Partition __transaction_state-46 broker=0] Log loaded for partition __transaction_state-46 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,138] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-46 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,139] INFO [MergedLog partition=__transaction_state-46, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,142] INFO [MergedLog partition=__transaction_state-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,143] INFO Created log for partition __transaction_state-5 in /tmp/kafka-logs/__transaction_state-5 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,143] INFO [Partition __transaction_state-5 broker=0] No checkpointed highwatermark is found for partition __transaction_state-5 (kafka.cluster.Partition)
[2023-07-22 21:03:02,143] INFO [Partition __transaction_state-5 broker=0] Log loaded for partition __transaction_state-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,143] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-5 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,143] INFO [MergedLog partition=__transaction_state-5, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,147] INFO [MergedLog partition=__transaction_state-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,148] INFO Created log for partition __transaction_state-20 in /tmp/kafka-logs/__transaction_state-20 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,148] INFO [Partition __transaction_state-20 broker=0] No checkpointed highwatermark is found for partition __transaction_state-20 (kafka.cluster.Partition)
[2023-07-22 21:03:02,148] INFO [Partition __transaction_state-20 broker=0] Log loaded for partition __transaction_state-20 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,149] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-20 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,150] INFO [MergedLog partition=__transaction_state-20, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,152] INFO [MergedLog partition=__transaction_state-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,154] INFO Created log for partition __transaction_state-35 in /tmp/kafka-logs/__transaction_state-35 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,154] INFO [Partition __transaction_state-35 broker=0] No checkpointed highwatermark is found for partition __transaction_state-35 (kafka.cluster.Partition)
[2023-07-22 21:03:02,154] INFO [Partition __transaction_state-35 broker=0] Log loaded for partition __transaction_state-35 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,154] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-35 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,154] INFO [MergedLog partition=__transaction_state-35, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,158] INFO [MergedLog partition=__transaction_state-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,159] INFO Created log for partition __transaction_state-13 in /tmp/kafka-logs/__transaction_state-13 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,159] INFO [Partition __transaction_state-13 broker=0] No checkpointed highwatermark is found for partition __transaction_state-13 (kafka.cluster.Partition)
[2023-07-22 21:03:02,159] INFO [Partition __transaction_state-13 broker=0] Log loaded for partition __transaction_state-13 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,159] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-13 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,159] INFO [MergedLog partition=__transaction_state-13, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,162] INFO [MergedLog partition=__transaction_state-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,164] INFO Created log for partition __transaction_state-28 in /tmp/kafka-logs/__transaction_state-28 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,164] INFO [Partition __transaction_state-28 broker=0] No checkpointed highwatermark is found for partition __transaction_state-28 (kafka.cluster.Partition)
[2023-07-22 21:03:02,164] INFO [Partition __transaction_state-28 broker=0] Log loaded for partition __transaction_state-28 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,164] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-28 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,165] INFO [MergedLog partition=__transaction_state-28, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,168] INFO [MergedLog partition=__transaction_state-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,169] INFO Created log for partition __transaction_state-43 in /tmp/kafka-logs/__transaction_state-43 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,170] INFO [Partition __transaction_state-43 broker=0] No checkpointed highwatermark is found for partition __transaction_state-43 (kafka.cluster.Partition)
[2023-07-22 21:03:02,170] INFO [Partition __transaction_state-43 broker=0] Log loaded for partition __transaction_state-43 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,170] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-43 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,170] INFO [MergedLog partition=__transaction_state-43, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,173] INFO [MergedLog partition=__transaction_state-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,175] INFO Created log for partition __transaction_state-6 in /tmp/kafka-logs/__transaction_state-6 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,175] INFO [Partition __transaction_state-6 broker=0] No checkpointed highwatermark is found for partition __transaction_state-6 (kafka.cluster.Partition)
[2023-07-22 21:03:02,175] INFO [Partition __transaction_state-6 broker=0] Log loaded for partition __transaction_state-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,175] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-6 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,176] INFO [MergedLog partition=__transaction_state-6, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,176] INFO Creating topic __transaction_state with configuration {compression.type=uncompressed, cleanup.policy=compact, min.insync.replicas=1, segment.bytes=104857600, confluent.placement.constraints=, unclean.leader.election.enable=false} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-22 21:03:02,179] INFO [MergedLog partition=__transaction_state-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,181] INFO Created log for partition __transaction_state-21 in /tmp/kafka-logs/__transaction_state-21 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,182] INFO [Partition __transaction_state-21 broker=0] No checkpointed highwatermark is found for partition __transaction_state-21 (kafka.cluster.Partition)
[2023-07-22 21:03:02,182] INFO [Partition __transaction_state-21 broker=0] Log loaded for partition __transaction_state-21 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,182] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-21 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,182] INFO [MergedLog partition=__transaction_state-21, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,187] INFO [MergedLog partition=__transaction_state-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,189] INFO Created log for partition __transaction_state-36 in /tmp/kafka-logs/__transaction_state-36 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,189] INFO [Partition __transaction_state-36 broker=0] No checkpointed highwatermark is found for partition __transaction_state-36 (kafka.cluster.Partition)
[2023-07-22 21:03:02,189] INFO [Partition __transaction_state-36 broker=0] Log loaded for partition __transaction_state-36 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,189] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-36 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,189] INFO [MergedLog partition=__transaction_state-36, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,193] INFO [MergedLog partition=__transaction_state-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,194] INFO Created log for partition __transaction_state-14 in /tmp/kafka-logs/__transaction_state-14 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,194] INFO [Partition __transaction_state-14 broker=0] No checkpointed highwatermark is found for partition __transaction_state-14 (kafka.cluster.Partition)
[2023-07-22 21:03:02,195] INFO [Partition __transaction_state-14 broker=0] Log loaded for partition __transaction_state-14 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,195] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-14 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,195] INFO [MergedLog partition=__transaction_state-14, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,198] INFO [MergedLog partition=__transaction_state-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,200] INFO Created log for partition __transaction_state-29 in /tmp/kafka-logs/__transaction_state-29 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,200] INFO [Partition __transaction_state-29 broker=0] No checkpointed highwatermark is found for partition __transaction_state-29 (kafka.cluster.Partition)
[2023-07-22 21:03:02,200] INFO [Partition __transaction_state-29 broker=0] Log loaded for partition __transaction_state-29 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,200] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-29 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,201] INFO [MergedLog partition=__transaction_state-29, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,204] INFO [MergedLog partition=__transaction_state-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:03:02,205] INFO Created log for partition __transaction_state-44 in /tmp/kafka-logs/__transaction_state-44 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-22 21:03:02,205] INFO [Partition __transaction_state-44 broker=0] No checkpointed highwatermark is found for partition __transaction_state-44 (kafka.cluster.Partition)
[2023-07-22 21:03:02,205] INFO [Partition __transaction_state-44 broker=0] Log loaded for partition __transaction_state-44 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:03:02,205] INFO Setting topicIdPartition 4o8tx8w1TBiaTRCD_GjQyA:__transaction_state-44 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:03:02,205] INFO [MergedLog partition=__transaction_state-44, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:03:02,208] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 3 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,221] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 18 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,221] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 33 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,221] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 11 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,221] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 26 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,221] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 41 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,222] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 4 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,222] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 19 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,222] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 34 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,222] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 49 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,222] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 12 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,222] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 27 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,222] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 42 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,222] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-3 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,222] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 24 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,222] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 39 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,222] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 1 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,222] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 32 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 47 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 9 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 40 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 2 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 17 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 48 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 10 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 25 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 7 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 22 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 37 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 0 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 15 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 30 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 45 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,223] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 8 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,224] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 23 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,224] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 38 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,224] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 16 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,224] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 31 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,224] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 46 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,224] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 5 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,224] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 20 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,224] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 35 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,224] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 13 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,224] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 28 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,224] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 43 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,225] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 6 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,225] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 21 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,225] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 36 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,225] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 14 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,225] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 29 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,225] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 44 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:03:02,228] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-3 in 9 milliseconds, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,239] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-3 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,239] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-18 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,241] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-18 in 20 milliseconds, of which 18 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,241] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-18 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,241] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-33 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,243] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-33 in 22 milliseconds, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,243] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-33 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,243] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-11 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,245] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-11 in 24 milliseconds, of which 22 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,245] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-11 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,245] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-26 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,247] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-26 in 26 milliseconds, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,247] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-26 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,247] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-41 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,249] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-41 in 28 milliseconds, of which 26 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,249] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-41 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,249] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-4 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,251] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-4 in 29 milliseconds, of which 27 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,251] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-4 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,251] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-19 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,253] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-19 in 31 milliseconds, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,253] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-19 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,253] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-34 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,254] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-34 in 32 milliseconds, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,254] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-34 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,254] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-49 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,256] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-49 in 34 milliseconds, of which 32 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,256] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-49 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,256] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-12 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,257] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-12 in 35 milliseconds, of which 34 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,257] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-12 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,257] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-27 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,259] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-27 in 37 milliseconds, of which 35 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,259] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-27 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,259] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-42 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,261] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-42 in 39 milliseconds, of which 37 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,261] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-42 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,261] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-24 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,262] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-24 in 40 milliseconds, of which 39 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,262] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-24 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,262] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-39 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,264] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-39 in 42 milliseconds, of which 40 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,264] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-39 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,264] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-1 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,265] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-1 in 43 milliseconds, of which 42 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,265] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-1 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,265] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-32 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,267] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-32 in 45 milliseconds, of which 43 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,267] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-32 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,267] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-47 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,269] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-47 in 46 milliseconds, of which 44 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,269] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-47 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,269] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-9 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,271] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-9 in 48 milliseconds, of which 46 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,271] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-9 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,271] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-40 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,273] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-40 in 50 milliseconds, of which 48 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,273] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-40 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,273] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-2 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,274] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-2 in 51 milliseconds, of which 50 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,275] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-2 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,275] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-17 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,276] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-17 in 53 milliseconds, of which 52 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,276] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-17 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,276] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-48 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,278] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-48 in 55 milliseconds, of which 53 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,278] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-48 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,278] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-10 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,279] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-10 in 56 milliseconds, of which 55 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,279] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-10 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,279] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-25 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,281] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-25 in 58 milliseconds, of which 56 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,281] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-25 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,281] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-7 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,282] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-7 in 59 milliseconds, of which 58 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,283] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-7 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,283] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-22 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,284] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-22 in 61 milliseconds, of which 60 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,284] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-22 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,284] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-37 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,287] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-37 in 64 milliseconds, of which 61 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,287] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-37 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,288] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-0 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,290] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-0 in 67 milliseconds, of which 65 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,290] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-0 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,290] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-15 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,293] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-15 in 70 milliseconds, of which 67 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,293] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-15 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,293] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-30 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,295] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-30 in 72 milliseconds, of which 70 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,295] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-30 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,295] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-45 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,297] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-45 in 74 milliseconds, of which 72 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,297] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-45 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,297] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-8 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,299] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-8 in 75 milliseconds, of which 73 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,299] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-8 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,299] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-23 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,301] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-23 in 77 milliseconds, of which 75 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,301] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-23 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,301] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-38 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,304] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-38 in 80 milliseconds, of which 77 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,304] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-38 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,304] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-16 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,306] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-16 in 82 milliseconds, of which 80 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,306] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-16 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,306] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-31 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,308] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-31 in 84 milliseconds, of which 82 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,309] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-31 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,309] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-46 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,311] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-46 in 87 milliseconds, of which 85 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,311] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-46 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,311] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-5 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,312] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-5 in 88 milliseconds, of which 87 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,313] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-5 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,313] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-20 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,314] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-20 in 90 milliseconds, of which 89 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,314] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-20 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,314] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-35 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,316] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-35 in 92 milliseconds, of which 90 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,317] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-35 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,318] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-13 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,321] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-13 in 96 milliseconds, of which 94 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,321] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-13 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,321] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-28 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,339] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-28 in 115 milliseconds, of which 97 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,339] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-28 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,339] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-43 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,343] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-43 in 118 milliseconds, of which 114 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,343] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-43 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,343] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-6 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,345] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-6 in 120 milliseconds, of which 118 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,345] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-6 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,345] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-21 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,347] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-21 in 122 milliseconds, of which 120 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,347] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-21 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,347] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-36 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,349] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-36 in 124 milliseconds, of which 122 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,349] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-36 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,349] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-14 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,350] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-14 in 125 milliseconds, of which 124 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,351] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-14 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,351] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-29 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,352] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-29 in 127 milliseconds, of which 126 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,352] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-29 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,352] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-44 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,354] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-44 in 129 milliseconds, of which 127 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,354] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-44 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-22 21:03:02,428] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 0 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:04:43,797] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 7756 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:04:43,797] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 7660 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:04:43,797] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8978 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:04:43,797] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 7652 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:04:43,797] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8148 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:04:43,797] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9673 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:04:43,797] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9013 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:04:43,797] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9292 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:04:43,797] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8860 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:04:43,797] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8687 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:04:43,798] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9599 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:04:43,798] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8786 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:05:23,042] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:07:43,793] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 7792 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:07:43,793] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 7695 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:07:43,793] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9154 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:07:43,793] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 7911 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:07:43,793] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8274 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:07:43,793] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9824 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:07:43,793] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9107 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:07:43,793] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9379 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:07:43,793] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9030 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:07:43,793] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8772 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:07:43,793] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9655 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:07:43,793] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8918 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:10:24,136] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:10:43,779] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 7925 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:10:43,779] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 7789 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:10:43,779] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9238 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:10:43,779] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8152 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:10:43,779] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8408 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:10:43,779] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9941 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:10:43,779] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9202 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:10:43,779] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9530 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:10:43,779] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9158 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:10:43,779] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8902 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:10:43,779] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9731 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:10:43,779] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9005 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:11:21,658] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 1 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:12:26,054] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 2 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:12:26,473] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in Empty state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-2-consumer-200a20b0-ec2f-4c81-9558-da757629a6f4 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:12:26,474] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in Empty state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-3-consumer-53dfcf80-8551-476c-acb7-cee9b79e7ccf and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:12:26,474] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in Empty state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-1-consumer-4230fd7a-9ee7-4174-b32a-37bb885a8a98 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:12:26,474] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in Empty state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-4-consumer-fc0f59dc-79bc-416c-bd0a-e056b8f4ce38 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:12:26,476] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in state PreparingRebalance with old generation 0 (__consumer_offsets-20) (reason: Adding new member _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-4-consumer-fc0f59dc-79bc-416c-bd0a-e056b8f4ce38 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:12:26,482] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 generation 1 (__consumer_offsets-20) with 4 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:12:26,510] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-4-consumer-fc0f59dc-79bc-416c-bd0a-e056b8f4ce38 for group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 for generation 1. The group has 4 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:12:38,642] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_transient_transient_TARGET_AVRO_282430637132372220_1690035158532 in Empty state. Created a new member id _confluent-ksql-default_transient_transient_TARGET_AVRO_282430637132372220_1690035158532-699d85e7-d3d1-4300-bf0e-d12da303febc-StreamThread-1-consumer-add8f74c-b86d-4d91-becd-0e5ac6a7f2f4 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:12:38,644] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_transient_transient_TARGET_AVRO_282430637132372220_1690035158532 in state PreparingRebalance with old generation 0 (__consumer_offsets-35) (reason: Adding new member _confluent-ksql-default_transient_transient_TARGET_AVRO_282430637132372220_1690035158532-699d85e7-d3d1-4300-bf0e-d12da303febc-StreamThread-1-consumer-add8f74c-b86d-4d91-becd-0e5ac6a7f2f4 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:12:38,644] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_transient_transient_TARGET_AVRO_282430637132372220_1690035158532 generation 1 (__consumer_offsets-35) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:12:38,652] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_transient_transient_TARGET_AVRO_282430637132372220_1690035158532-699d85e7-d3d1-4300-bf0e-d12da303febc-StreamThread-1-consumer-add8f74c-b86d-4d91-becd-0e5ac6a7f2f4 for group _confluent-ksql-default_transient_transient_TARGET_AVRO_282430637132372220_1690035158532 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:13:24,066] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_transient_transient_TARGET_AVRO_282430637132372220_1690035158532-699d85e7-d3d1-4300-bf0e-d12da303febc-StreamThread-1-consumer-add8f74c-b86d-4d91-becd-0e5ac6a7f2f4 in group _confluent-ksql-default_transient_transient_TARGET_AVRO_282430637132372220_1690035158532 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:13:24,067] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_transient_transient_TARGET_AVRO_282430637132372220_1690035158532 in state PreparingRebalance with old generation 1 (__consumer_offsets-35) (reason: removing member _confluent-ksql-default_transient_transient_TARGET_AVRO_282430637132372220_1690035158532-699d85e7-d3d1-4300-bf0e-d12da303febc-StreamThread-1-consumer-add8f74c-b86d-4d91-becd-0e5ac6a7f2f4 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:13:24,068] INFO [GroupCoordinator 0]: Group _confluent-ksql-default_transient_transient_TARGET_AVRO_282430637132372220_1690035158532 with generation 2 is now empty (__consumer_offsets-35) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:13:24,181] INFO [GroupCoordinator 0]: The following groups were deleted: _confluent-ksql-default_transient_transient_TARGET_AVRO_282430637132372220_1690035158532. A total of 1 offsets were removed. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:13:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8048 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:13:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 7904 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:13:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9284 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:13:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8255 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:13:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8519 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:13:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 10075 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:13:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9282 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:13:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9649 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:13:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9376 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:13:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9000 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:13:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9922 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:13:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9137 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:15:25,238] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:16:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8301 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:16:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8069 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:16:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9417 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:16:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8255 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:16:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 8672 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:16:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 10174 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:16:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9355 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:16:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9797 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:16:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9528 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:16:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9155 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:16:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9961 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:16:43,778] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Seeking to offset 9251 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:31,044] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-0, groupId=null] Disconnecting from node 0 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:18:31,143] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-0, groupId=null] Cancelled in-flight FETCH request with correlation id 32118 due to node 0 being disconnected (elapsed time since creation: 30776ms, elapsed time since send: 30776ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:18:31,147] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-0, groupId=null] Error sending fetch request (sessionId=773485842, epoch=32059) to node 0: (org.apache.kafka.clients.FetchSessionHandler)
org.apache.kafka.common.errors.DisconnectException
[2023-07-22 21:18:31,152] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Disconnecting from node 0 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:18:31,152] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Cancelled in-flight PRODUCE request with correlation id 2535 due to node 0 being disconnected (elapsed time since creation: 44464ms, elapsed time since send: 44464ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:18:31,138] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Disconnecting from node 2147483647 due to request timeout. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:18:31,153] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Cancelled in-flight HEARTBEAT request with correlation id 5798 due to node 2147483647 being disconnected (elapsed time since creation: 33743ms, elapsed time since send: 32785ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:18:31,153] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Cancelled in-flight HEARTBEAT request with correlation id 5799 due to node 2147483647 being disconnected (elapsed time since creation: 28717ms, elapsed time since send: 28243ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:18:31,153] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Cancelled in-flight HEARTBEAT request with correlation id 5800 due to node 2147483647 being disconnected (elapsed time since creation: 23316ms, elapsed time since send: 22220ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:18:31,153] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Cancelled in-flight HEARTBEAT request with correlation id 5801 due to node 2147483647 being disconnected (elapsed time since creation: 21077ms, elapsed time since send: 21072ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:18:31,153] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Cancelled in-flight HEARTBEAT request with correlation id 5802 due to node 2147483647 being disconnected (elapsed time since creation: 18560ms, elapsed time since send: 17906ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:18:31,153] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Cancelled in-flight HEARTBEAT request with correlation id 5803 due to node 2147483647 being disconnected (elapsed time since creation: 5883ms, elapsed time since send: 5871ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:18:31,138] INFO [GroupCoordinator 0]: Member sr-1-4a8ad623-326a-4334-85ce-756604e9bafd in group schema-registry has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:31,155] INFO [GroupCoordinator 0]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: removing member sr-1-4a8ad623-326a-4334-85ce-756604e9bafd on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:31,155] INFO [GroupCoordinator 0]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:31,055] WARN Client session timed out, have not heard from server in 12015ms for session id 0x10000020b2b0000 (org.apache.zookeeper.ClientCnxn)
[2023-07-22 21:18:31,159] WARN Session 0x10000020b2b0000 for server localhost/127.0.0.1:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException. (org.apache.zookeeper.ClientCnxn)
org.apache.zookeeper.ClientCnxn$SessionTimeoutException: Client session timed out, have not heard from server in 12015ms for session id 0x10000020b2b0000
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1261)
[2023-07-22 21:18:31,201] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Got error produce response with correlation id 2535 on topic-partition _confluent-telemetry-metrics-2, retrying (2147483646 attempts left). Error: NETWORK_EXCEPTION. Error Message: Disconnected from node 0 (org.apache.kafka.clients.producer.internals.Sender)
[2023-07-22 21:18:31,175] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Group coordinator localhost:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: null. isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:31,175] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-1-consumer-4230fd7a-9ee7-4174-b32a-37bb885a8a98 in group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:31,222] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in state PreparingRebalance with old generation 1 (__consumer_offsets-20) (reason: removing member _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-1-consumer-4230fd7a-9ee7-4174-b32a-37bb885a8a98 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:31,222] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-3-consumer-53dfcf80-8551-476c-acb7-cee9b79e7ccf in group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:31,233] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Received invalid metadata error in produce request on partition _confluent-telemetry-metrics-2 due to org.apache.kafka.common.errors.NetworkException: Disconnected from node 0. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender)
[2023-07-22 21:18:31,236] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-4bc715ae-dbd9-430f-9de3-10534cf8d63e and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:31,267] INFO [GroupCoordinator 0]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 2 (__consumer_offsets-29) (reason: Adding new member sr-1-4bc715ae-dbd9-430f-9de3-10534cf8d63e with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:31,274] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 generation 2 (__consumer_offsets-20) with 2 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:31,295] INFO [GroupCoordinator 0]: Stabilized group schema-registry generation 3 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:31,339] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2023-07-22 21:18:31,340] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[2023-07-22 21:18:31,465] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-4-consumer-fc0f59dc-79bc-416c-bd0a-e056b8f4ce38 for group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 for generation 2. The group has 2 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:31,514] INFO [GroupCoordinator 0]: Assignment received from leader sr-1-4bc715ae-dbd9-430f-9de3-10534cf8d63e for group schema-registry for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:31,573] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:31,581] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:31,611] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in Stable state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-3-consumer-7666bdc7-ed32-4556-869e-a392f43f08db and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:31,616] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in state PreparingRebalance with old generation 2 (__consumer_offsets-20) (reason: Adding new member _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-3-consumer-7666bdc7-ed32-4556-869e-a392f43f08db with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:31,618] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in PreparingRebalance state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-1-consumer-d51a820d-8003-4969-8725-f8a11fde8116 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:33,216] INFO Opening socket connection to server localhost/127.0.0.1:2181. (org.apache.zookeeper.ClientCnxn)
[2023-07-22 21:18:33,217] INFO Socket connection established, initiating session, client: /127.0.0.1:59074, server: localhost/127.0.0.1:2181 (org.apache.zookeeper.ClientCnxn)
[2023-07-22 21:18:33,333] WARN Unable to reconnect to ZooKeeper service, session 0x10000020b2b0000 has expired (org.apache.zookeeper.ClientCnxn)
[2023-07-22 21:18:33,333] WARN Session 0x10000020b2b0000 for server localhost/127.0.0.1:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException. (org.apache.zookeeper.ClientCnxn)
org.apache.zookeeper.ClientCnxn$SessionExpiredException: Unable to reconnect to ZooKeeper service, session 0x10000020b2b0000 has expired
	at org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1438)
	at org.apache.zookeeper.ClientCnxnSocket.readConnectResult(ClientCnxnSocket.java:154)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:86)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1293)
[2023-07-22 21:18:33,346] INFO EventThread shut down for session: 0x10000020b2b0000 (org.apache.zookeeper.ClientCnxn)
[2023-07-22 21:18:33,357] INFO [ZooKeeperClient Kafka server] Session expired. (kafka.zookeeper.ZooKeeperClient)
[2023-07-22 21:18:33,383] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2023-07-22 21:18:33,383] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@42c2f48c (org.apache.zookeeper.ZooKeeper)
[2023-07-22 21:18:33,388] INFO Partition SLO metrics processing controller change event for the broker 0 with flags [isActiveController: false, isMultiTenant: false] (kafka.Kafka$)
[2023-07-22 21:18:33,388] INFO Unregistering the Partition SLO metrics from the broker 0 (kafka.Kafka$)
[2023-07-22 21:18:33,410] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
[2023-07-22 21:18:33,411] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)
[2023-07-22 21:18:33,449] INFO Opening socket connection to server localhost/127.0.0.1:2181. (org.apache.zookeeper.ClientCnxn)
[2023-07-22 21:18:33,450] INFO Socket connection established, initiating session, client: /127.0.0.1:59078, server: localhost/127.0.0.1:2181 (org.apache.zookeeper.ClientCnxn)
[2023-07-22 21:18:33,464] ERROR Error processing describe configs request for resource DescribeConfigsResource(resourceType=2, resourceName='__transaction_state', configurationKeys=null) (kafka.server.ConfigHelper)
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /config/topics/__transaction_state
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:134)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:54)
	at kafka.zookeeper.AsyncResponse.resultException(ZooKeeperClient.scala:559)
	at kafka.zk.KafkaZkClient.getEntityConfigs(KafkaZkClient.scala:440)
	at kafka.zk.AdminZkClient.fetchEntityConfig(AdminZkClient.scala:609)
	at kafka.server.metadata.ZkConfigRepository.config(ZkConfigRepository.scala:48)
	at kafka.server.metadata.ConfigRepository.topicConfig(ConfigRepository.scala:33)
	at kafka.server.metadata.ConfigRepository.topicConfig$(ConfigRepository.scala:32)
	at kafka.server.metadata.ZkConfigRepository.topicConfig(ZkConfigRepository.scala:33)
	at kafka.server.ConfigHelper.$anonfun$describeConfigs$1(ConfigHelper.scala:68)
	at scala.collection.immutable.List.map(List.scala:246)
	at kafka.server.ConfigHelper.describeConfigs(ConfigHelper.scala:51)
	at kafka.server.KafkaApis.handleDescribeConfigsRequest(KafkaApis.scala:3749)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:297)
	at kafka.server.KafkaRequestHandler.$anonfun$poll$3(KafkaRequestHandler.scala:124)
	at kafka.server.KafkaRequestHandler.$anonfun$poll$3$adapted(KafkaRequestHandler.scala:124)
	at io.confluent.kafka.availability.ThreadCountersManager.wrapEngine(ThreadCountersManager.java:146)
	at kafka.server.KafkaRequestHandler.poll(KafkaRequestHandler.scala:124)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:96)
	at java.base/java.lang.Thread.run(Thread.java:829)
	at org.apache.kafka.common.utils.KafkaThread.run(KafkaThread.java:64)
[2023-07-22 21:18:33,509] INFO Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x10000020b2b0001, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
[2023-07-22 21:18:33,527] INFO Processing notification(s) to /config/changes (kafka.common.ZkNodeChangeNotificationListener)
[2023-07-22 21:18:33,547] INFO [MetadataCache brokerId=0] Updated cache from existing FinalizedFeaturesAndEpoch(features=Map(), epoch=0) to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0). (kafka.server.metadata.ZkMetadataCache)
[2023-07-22 21:18:33,569] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2023-07-22 21:18:33,593] INFO Stat of the created znode at /brokers/ids/0 is: 387,387,1690035513582,1690035513582,1,0,0,72057602815229953,212,0,387
 (kafka.zk.KafkaZkClient)
[2023-07-22 21:18:33,593] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:9092, czxid (broker epoch): 387 (kafka.zk.KafkaZkClient)
[2023-07-22 21:18:33,626] INFO [Producer clientId=_confluent_balancer_api_state-producer-0] Resetting the last seen epoch of partition _confluent_balancer_api_state-0 to 0 since the associated topicId changed from null to 1f-sUzU-TP2IPoKgHBGBmA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:33,689] INFO Partition SLO metrics processing controller change event for the broker 0 with flags [isActiveController: true, isMultiTenant: false] (kafka.Kafka$)
[2023-07-22 21:18:33,730] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Revoke previously assigned partitions _confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2, _confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:33,754] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Member kafka-cruise-control-1bfef6fd-06cd-4be1-9055-a7595c8d7cb5 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:33,755] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:33,755] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:33,755] WARN [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--7704318691715494834] Close timed out with 1 pending requests to coordinator, terminating client connections (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:33,757] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:33,757] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:33,757] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:33,759] INFO App info kafka.consumer for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:33,760] INFO [GroupCoordinator 0]: Preparing to rebalance group ConfluentTelemetryReporterSampler--7704318691715494834 in state PreparingRebalance with old generation 1 (__consumer_offsets-13) (reason: Removing member kafka-cruise-control-1bfef6fd-06cd-4be1-9055-a7595c8d7cb5 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:33,760] INFO [GroupCoordinator 0]: Group ConfluentTelemetryReporterSampler--7704318691715494834 with generation 2 is now empty (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:33,764] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=kafka-cruise-control-1bfef6fd-06cd-4be1-9055-a7595c8d7cb5, groupInstanceId=None, clientId=kafka-cruise-control, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=2147483647, supportedProtocols=List(range, cooperative-sticky)) has left group ConfluentTelemetryReporterSampler--7704318691715494834 through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:34,348] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 generation 3 (__consumer_offsets-20) with 4 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:34,383] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-4-consumer-fc0f59dc-79bc-416c-bd0a-e056b8f4ce38 for group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 for generation 3. The group has 4 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:34,454] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in state PreparingRebalance with old generation 3 (__consumer_offsets-20) (reason: Updating metadata for member _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-2-consumer-200a20b0-ec2f-4c81-9558-da757629a6f4 during Stable; client reason: need to revoke partitions and re-join) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:34,685] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,691] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,691] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,691] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,692] INFO Stopping KafkaBasedLog for topic _confluent_balancer_api_state (org.apache.kafka.connect.util.KafkaBasedLog)
[2023-07-22 21:18:34,693] INFO [Producer clientId=_confluent_balancer_api_state-producer-0] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2023-07-22 21:18:34,698] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,698] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,698] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,698] INFO App info kafka.producer for _confluent_balancer_api_state-producer-0 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,698] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,698] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,698] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,700] INFO App info kafka.consumer for _confluent_balancer_api_state-consumer-0 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,700] INFO Stopped KafkaBasedLog for topic _confluent_balancer_api_state (org.apache.kafka.connect.util.KafkaBasedLog)
[2023-07-22 21:18:34,700] INFO App info kafka.admin.client for null-admin-0 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,702] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,702] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,702] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,737] WARN Attempted to fetch endpoint for listener ListenerName(PLAINTEXT) from AliveBrokersSnapshot but AliveBrokersSnapshot does not have this information. (kafka.common.AliveBrokersSnapshot)
[2023-07-22 21:18:34,762] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-cruise-control
	client.rack = 
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ConfluentTelemetryReporterSampler-1030259255596345382
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 2147483647
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 50
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig)
[2023-07-22 21:18:34,777] WARN These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.login.refresh.window.factor, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2023-07-22 21:18:34,777] INFO Kafka version: 7.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,777] INFO Kafka commitId: cc2964509eb513f9fcd13f7d8d80bba29fedc13a (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,777] INFO Kafka startTimeMs: 1690035514777 (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,777] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Subscribed to pattern: '_confluent-telemetry-metrics' (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:34,782] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Resetting the last seen epoch of partition _confluent-telemetry-metrics-0 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:34,782] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Resetting the last seen epoch of partition _confluent-telemetry-metrics-5 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:34,782] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Resetting the last seen epoch of partition _confluent-telemetry-metrics-10 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:34,782] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Resetting the last seen epoch of partition _confluent-telemetry-metrics-8 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:34,782] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Resetting the last seen epoch of partition _confluent-telemetry-metrics-2 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:34,782] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Resetting the last seen epoch of partition _confluent-telemetry-metrics-9 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:34,782] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Resetting the last seen epoch of partition _confluent-telemetry-metrics-11 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:34,782] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Resetting the last seen epoch of partition _confluent-telemetry-metrics-4 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:34,782] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Resetting the last seen epoch of partition _confluent-telemetry-metrics-1 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:34,782] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Resetting the last seen epoch of partition _confluent-telemetry-metrics-6 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:34,782] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Resetting the last seen epoch of partition _confluent-telemetry-metrics-7 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:34,782] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Resetting the last seen epoch of partition _confluent-telemetry-metrics-3 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:34,783] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Cluster ID: -63FH6-5RTqDouE2Fhrgug (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:34,784] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:34,784] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-1030259255596345382] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:34,784] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,784] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,784] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,786] INFO App info kafka.consumer for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,787] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-cruise-control
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	confluent.use.controller.listener = false
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 5000
	reconnect.backoff.ms = 500
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2023-07-22 21:18:34,802] WARN These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.login.refresh.window.factor, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
[2023-07-22 21:18:34,802] INFO Kafka version: 7.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,802] INFO Kafka commitId: cc2964509eb513f9fcd13f7d8d80bba29fedc13a (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,802] INFO Kafka startTimeMs: 1690035514802 (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,845] INFO App info kafka.admin.client for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,846] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,846] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,846] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:34,847] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-cruise-control
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	confluent.use.controller.listener = false
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 5000
	reconnect.backoff.ms = 500
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2023-07-22 21:18:34,852] WARN These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.login.refresh.window.factor, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
[2023-07-22 21:18:34,852] INFO Kafka version: 7.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,852] INFO Kafka commitId: cc2964509eb513f9fcd13f7d8d80bba29fedc13a (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,852] INFO Kafka startTimeMs: 1690035514852 (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:34,921] INFO [Admin Manager on Broker 0]: User:ANONYMOUS is updating topic _confluent_balancer_api_state with new configuration : cleanup.policy -> compact,retention.ms -> -1 (kafka.server.ZkAdminManager)
[2023-07-22 21:18:34,949] INFO Processing notification(s) to /config/changes (kafka.common.ZkNodeChangeNotificationListener)
[2023-07-22 21:18:35,002] INFO App info kafka.admin.client for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,002] INFO Processing override for entityPath: topics/_confluent_balancer_api_state with config: HashMap(cleanup.policy -> compact, retention.ms -> -1) (kafka.server.ZkConfigManager)
[2023-07-22 21:18:35,003] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:35,003] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:35,003] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[2023-07-22 21:18:35,113] WARN Attempted to fetch endpoint for listener ListenerName(PLAINTEXT) from AliveBrokersSnapshot but AliveBrokersSnapshot does not have this information. (kafka.common.AliveBrokersSnapshot)
[2023-07-22 21:18:35,115] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = null-admin-0
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	confluent.use.controller.listener = false
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 5000
	reconnect.backoff.ms = 500
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2023-07-22 21:18:35,120] INFO Kafka version: 7.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,120] INFO Kafka commitId: cc2964509eb513f9fcd13f7d8d80bba29fedc13a (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,120] INFO Kafka startTimeMs: 1690035515119 (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,123] INFO Starting KafkaBasedLog with topic _confluent_balancer_api_state (org.apache.kafka.connect.util.KafkaBasedLog)
[2023-07-22 21:18:35,128] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent_balancer_api_state-producer-0
	compression.type = none
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class io.confluent.databalancer.persistence.ApiStatePersistenceStore$SbkApiStatusKeySerde
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.databalancer.persistence.ApiStatePersistenceStore$SbkApiStatusMessageSerde
 (org.apache.kafka.clients.producer.ProducerConfig)
[2023-07-22 21:18:35,164] INFO Kafka version: 7.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,167] INFO Kafka commitId: cc2964509eb513f9fcd13f7d8d80bba29fedc13a (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,167] INFO Kafka startTimeMs: 1690035515164 (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,168] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent_balancer_api_state-consumer-0
	client.rack = 
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.databalancer.persistence.ApiStatePersistenceStore$SbkApiStatusKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.databalancer.persistence.ApiStatePersistenceStore$SbkApiStatusMessageSerde
 (org.apache.kafka.clients.consumer.ConsumerConfig)
[2023-07-22 21:18:35,170] INFO [Producer clientId=_confluent_balancer_api_state-producer-0] Cluster ID: -63FH6-5RTqDouE2Fhrgug (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,179] INFO Kafka version: 7.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,179] INFO Kafka commitId: cc2964509eb513f9fcd13f7d8d80bba29fedc13a (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,179] INFO Kafka startTimeMs: 1690035515179 (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,186] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-0, groupId=null] Cluster ID: -63FH6-5RTqDouE2Fhrgug (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,186] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-0, groupId=null] Assigned to partition(s): _confluent_balancer_api_state-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:35,187] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-0, groupId=null] Seeking to earliest offset of partition _confluent_balancer_api_state-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2023-07-22 21:18:35,246] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-0, groupId=null] Resetting the last seen epoch of partition _confluent_balancer_api_state-0 to 0 since the associated topicId changed from null to 1f-sUzU-TP2IPoKgHBGBmA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,281] INFO Finished reading KafkaBasedLog for topic _confluent_balancer_api_state (org.apache.kafka.connect.util.KafkaBasedLog)
[2023-07-22 21:18:35,281] INFO Started KafkaBasedLog for topic _confluent_balancer_api_state (org.apache.kafka.connect.util.KafkaBasedLog)
[2023-07-22 21:18:35,282] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	confluent.use.controller.listener = false
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 5000
	reconnect.backoff.ms = 500
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2023-07-22 21:18:35,289] INFO Kafka version: 7.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,289] INFO Kafka commitId: cc2964509eb513f9fcd13f7d8d80bba29fedc13a (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,289] INFO Kafka startTimeMs: 1690035515289 (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,556] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = kafka-cruise-control
	client.rack = 
	confluent.proxy.protocol.client.address = null
	confluent.proxy.protocol.client.port = null
	confluent.proxy.protocol.client.version = NONE
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ConfluentTelemetryReporterSampler-9022899771897038200
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 2147483647
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 50
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig)
[2023-07-22 21:18:35,563] WARN These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.login.refresh.window.factor, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2023-07-22 21:18:35,564] INFO Kafka version: 7.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,564] INFO Kafka commitId: cc2964509eb513f9fcd13f7d8d80bba29fedc13a (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,564] INFO Kafka startTimeMs: 1690035515564 (org.apache.kafka.common.utils.AppInfoParser)
[2023-07-22 21:18:35,564] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Subscribed to pattern: '_confluent-telemetry-metrics' (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Resetting the last seen epoch of partition _confluent-telemetry-metrics-0 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Resetting the last seen epoch of partition _confluent-telemetry-metrics-5 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Resetting the last seen epoch of partition _confluent-telemetry-metrics-10 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Resetting the last seen epoch of partition _confluent-telemetry-metrics-8 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Resetting the last seen epoch of partition _confluent-telemetry-metrics-2 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Resetting the last seen epoch of partition _confluent-telemetry-metrics-9 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Resetting the last seen epoch of partition _confluent-telemetry-metrics-11 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Resetting the last seen epoch of partition _confluent-telemetry-metrics-4 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Resetting the last seen epoch of partition _confluent-telemetry-metrics-1 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Resetting the last seen epoch of partition _confluent-telemetry-metrics-6 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Resetting the last seen epoch of partition _confluent-telemetry-metrics-7 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Resetting the last seen epoch of partition _confluent-telemetry-metrics-3 to 0 since the associated topicId changed from null to AV1QHDwwTaGwQuj9EThqZA (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Cluster ID: -63FH6-5RTqDouE2Fhrgug (org.apache.kafka.clients.Metadata)
[2023-07-22 21:18:35,676] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,685] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,689] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group ConfluentTelemetryReporterSampler-9022899771897038200 in Empty state. Created a new member id kafka-cruise-control-1088a1a9-d074-4834-b07b-48cc54ad9b61 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:35,690] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Request joining group due to: need to re-join with the given member-id: kafka-cruise-control-1088a1a9-d074-4834-b07b-48cc54ad9b61 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,690] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,690] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,691] INFO [GroupCoordinator 0]: Preparing to rebalance group ConfluentTelemetryReporterSampler-9022899771897038200 in state PreparingRebalance with old generation 0 (__consumer_offsets-32) (reason: Adding new member kafka-cruise-control-1088a1a9-d074-4834-b07b-48cc54ad9b61 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:35,693] INFO [GroupCoordinator 0]: Stabilized group ConfluentTelemetryReporterSampler-9022899771897038200 generation 1 (__consumer_offsets-32) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:35,694] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Successfully joined group with generation Generation{generationId=1, memberId='kafka-cruise-control-1088a1a9-d074-4834-b07b-48cc54ad9b61', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,694] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Finished assignment for group at generation 1: {kafka-cruise-control-1088a1a9-d074-4834-b07b-48cc54ad9b61=Assignment(partitions=[_confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2, _confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,696] INFO [GroupCoordinator 0]: Assignment received from leader kafka-cruise-control-1088a1a9-d074-4834-b07b-48cc54ad9b61 for group ConfluentTelemetryReporterSampler-9022899771897038200 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:35,699] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Successfully synced group in generation Generation{generationId=1, memberId='kafka-cruise-control-1088a1a9-d074-4834-b07b-48cc54ad9b61', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,699] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Notifying assignor about the new Assignment(partitions=[_confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2, _confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,699] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Adding newly assigned partitions: _confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2, _confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,702] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Found no committed offset for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,702] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Found no committed offset for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,702] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Found no committed offset for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,702] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Found no committed offset for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,702] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Found no committed offset for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,702] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Found no committed offset for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,702] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Found no committed offset for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,702] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Found no committed offset for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,702] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Found no committed offset for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,702] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Found no committed offset for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,702] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Found no committed offset for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:35,702] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Found no committed offset for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
[2023-07-22 21:18:36,242] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8301 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:36,242] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8069 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:36,242] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9417 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:36,242] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8310 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:36,242] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8672 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:36,242] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10174 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:36,242] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9607 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:36,242] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9849 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:36,242] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9613 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:36,242] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9155 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:36,242] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9961 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:36,242] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9304 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:18:37,381] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 generation 4 (__consumer_offsets-20) with 4 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:18:37,391] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-4-consumer-fc0f59dc-79bc-416c-bd0a-e056b8f4ce38 for group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 for generation 4. The group has 4 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:21:35,580] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8402 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:21:35,580] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8277 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:21:35,580] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9685 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:21:35,580] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8450 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:21:35,580] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8919 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:21:35,580] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10414 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:21:35,580] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9607 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:21:35,580] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9903 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:21:35,580] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9613 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:21:35,580] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9155 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:21:35,580] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9961 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:21:35,580] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9538 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:22:26,288] INFO [TransactionCoordinator id=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:23:35,258] INFO [AdminClient clientId=null-admin-0] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:23:35,348] INFO [AdminClient clientId=adminclient-2] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:24:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8506 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:24:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8427 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:24:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9724 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:24:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8568 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:24:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9123 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:24:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10556 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:24:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9646 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:24:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10061 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:24:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9729 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:24:35,579] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9443 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:24:35,579] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10054 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:24:35,579] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9578 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:27:35,251] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-0, groupId=null] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:27:35,361] INFO [Producer clientId=_confluent_balancer_api_state-producer-0] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:27:35,576] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:27:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8666 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:27:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8463 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:27:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9910 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:27:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8680 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:27:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9227 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:27:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10673 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:27:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9646 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:27:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10182 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:27:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9821 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:27:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9603 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:27:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10054 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:27:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9981 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:28:27,199] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group console-consumer-91177 in Empty state. Created a new member id console-consumer-0c9b77bd-fb57-4d0c-bddb-02c7f550c44e and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:28:27,202] INFO [GroupCoordinator 0]: Preparing to rebalance group console-consumer-91177 in state PreparingRebalance with old generation 0 (__consumer_offsets-6) (reason: Adding new member console-consumer-0c9b77bd-fb57-4d0c-bddb-02c7f550c44e with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:28:27,202] INFO [GroupCoordinator 0]: Stabilized group console-consumer-91177 generation 1 (__consumer_offsets-6) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:28:27,207] INFO [GroupCoordinator 0]: Assignment received from leader console-consumer-0c9b77bd-fb57-4d0c-bddb-02c7f550c44e for group console-consumer-91177 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:28:35,335] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:29:18,147] INFO [GroupCoordinator 0]: Preparing to rebalance group console-consumer-91177 in state PreparingRebalance with old generation 1 (__consumer_offsets-6) (reason: Removing member console-consumer-0c9b77bd-fb57-4d0c-bddb-02c7f550c44e on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:29:18,147] INFO [GroupCoordinator 0]: Group console-consumer-91177 with generation 2 is now empty (__consumer_offsets-6) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:29:18,148] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=console-consumer-0c9b77bd-fb57-4d0c-bddb-02c7f550c44e, groupInstanceId=None, clientId=console-consumer, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group console-consumer-91177 through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:29:29,534] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group console-consumer-68844 in Empty state. Created a new member id console-consumer-1d4438ab-421b-4e39-8ff3-fda0bb2b29c8 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:29:29,536] INFO [GroupCoordinator 0]: Preparing to rebalance group console-consumer-68844 in state PreparingRebalance with old generation 0 (__consumer_offsets-11) (reason: Adding new member console-consumer-1d4438ab-421b-4e39-8ff3-fda0bb2b29c8 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:29:29,537] INFO [GroupCoordinator 0]: Stabilized group console-consumer-68844 generation 1 (__consumer_offsets-11) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:29:29,545] INFO [GroupCoordinator 0]: Assignment received from leader console-consumer-1d4438ab-421b-4e39-8ff3-fda0bb2b29c8 for group console-consumer-68844 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:29:35,962] INFO [GroupCoordinator 0]: Preparing to rebalance group console-consumer-68844 in state PreparingRebalance with old generation 1 (__consumer_offsets-11) (reason: Removing member console-consumer-1d4438ab-421b-4e39-8ff3-fda0bb2b29c8 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:29:35,962] INFO [GroupCoordinator 0]: Group console-consumer-68844 with generation 2 is now empty (__consumer_offsets-11) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:29:35,963] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=console-consumer-1d4438ab-421b-4e39-8ff3-fda0bb2b29c8, groupInstanceId=None, clientId=console-consumer, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group console-consumer-68844 through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:30:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8804 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:30:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8631 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:30:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10013 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:30:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8800 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:30:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9348 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:30:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10755 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:30:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9738 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:30:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10387 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:30:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9961 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:30:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9689 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:30:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10146 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:30:35,578] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10125 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:33:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8966 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:33:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8858 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:33:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10193 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:33:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8884 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:33:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9348 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:33:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10907 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:33:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9968 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:33:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10477 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:33:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10060 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:33:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9819 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:33:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10233 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:33:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10175 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:33:36,435] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:35:11,789] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-mysql-sink in Empty state. Created a new member id connector-consumer-mysql-sink-0-1522a3c2-8814-4313-888e-8e9c4379df90 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:35:11,792] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-mysql-sink in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member connector-consumer-mysql-sink-0-1522a3c2-8814-4313-888e-8e9c4379df90 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:35:11,793] INFO [GroupCoordinator 0]: Stabilized group connect-mysql-sink generation 1 (__consumer_offsets-43) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:35:11,800] INFO [GroupCoordinator 0]: Assignment received from leader connector-consumer-mysql-sink-0-1522a3c2-8814-4313-888e-8e9c4379df90 for group connect-mysql-sink for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:35:33,697] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-mysql-sink in state PreparingRebalance with old generation 1 (__consumer_offsets-43) (reason: Removing member connector-consumer-mysql-sink-0-1522a3c2-8814-4313-888e-8e9c4379df90 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:35:33,697] INFO [GroupCoordinator 0]: Group connect-mysql-sink with generation 2 is now empty (__consumer_offsets-43) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:35:33,698] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connector-consumer-mysql-sink-0-1522a3c2-8814-4313-888e-8e9c4379df90, groupInstanceId=None, clientId=connector-consumer-mysql-sink-0, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group connect-mysql-sink through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:36:35,584] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9017 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:36:35,584] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8916 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:36:35,584] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10398 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:36:35,584] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8979 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:36:35,584] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9447 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:36:35,584] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10989 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:36:35,584] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10177 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:36:35,584] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10567 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:36:35,584] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10155 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:36:35,584] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10041 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:36:35,584] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10278 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:36:35,584] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10415 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:38:37,032] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:38:37,886] INFO [GroupMetadataManager brokerId=0] Group connect-mysql-sink transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2023-07-22 21:38:37,886] INFO [GroupMetadataManager brokerId=0] Group console-consumer-91177 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2023-07-22 21:38:37,887] INFO [GroupMetadataManager brokerId=0] Group console-consumer-68844 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2023-07-22 21:39:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9017 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:39:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9052 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:39:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10585 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:39:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 8979 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:39:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9556 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:39:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11186 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:39:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10395 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:39:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10630 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:39:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10229 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:39:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10200 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:39:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10444 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:39:35,577] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10597 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:40:22,366] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 3 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:40:24,507] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 4 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:40:32,042] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 5 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:40:39,465] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 6 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:40:48,286] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 7 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:40:52,362] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 8 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:41:00,361] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 9 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:41:04,611] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-3-consumer-7666bdc7-ed32-4556-869e-a392f43f08db in group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:41:04,612] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in state PreparingRebalance with old generation 4 (__consumer_offsets-20) (reason: removing member _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-3-consumer-7666bdc7-ed32-4556-869e-a392f43f08db on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:41:04,613] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-1-consumer-d51a820d-8003-4969-8725-f8a11fde8116 in group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:41:04,617] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-2-consumer-200a20b0-ec2f-4c81-9558-da757629a6f4 in group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:41:04,689] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-e25e6c47-9775-4be3-8816-536ec260b4d2-StreamThread-4-consumer-fc0f59dc-79bc-416c-bd0a-e056b8f4ce38 in group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:41:04,689] INFO [GroupCoordinator 0]: Group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 with generation 5 is now empty (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:41:07,164] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 10 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:41:07,674] INFO [GroupCoordinator 0]: The following groups were deleted: _confluent-ksql-default_query_CSAS_TARGET_AVRO_3. A total of 1 offsets were removed. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:42:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9017 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:42:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9401 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:42:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10659 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:42:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9090 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:42:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9693 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:42:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11231 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:42:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10480 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:42:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10810 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:42:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10445 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:42:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10268 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:42:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10537 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:42:35,574] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10730 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:43:38,130] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:44:22,220] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 11 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:44:26,603] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 12 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:44:39,354] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 13 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:44:45,858] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 14 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:45:31,093] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 15 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:45:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9105 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:45:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9489 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:45:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10709 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:45:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9180 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:45:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9693 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:45:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11309 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:45:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10620 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:45:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11014 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:45:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10671 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:45:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10395 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:45:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10705 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:45:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10962 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:45:43,858] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 16 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:45:44,106] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 in Empty state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_31-0f2d0c51-bb2b-49b7-85d7-f398d8398957-StreamThread-1-consumer-afa66494-6e48-4df5-9ab4-1cc60f04333d and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:45:44,107] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 in Empty state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_31-0f2d0c51-bb2b-49b7-85d7-f398d8398957-StreamThread-4-consumer-56a7eacd-f6e3-4bcd-ad3c-970cd11fedec and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:45:44,107] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 in Empty state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_31-0f2d0c51-bb2b-49b7-85d7-f398d8398957-StreamThread-3-consumer-dd6ce174-0286-4119-95f7-12f76d3fa31d and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:45:44,107] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 in state PreparingRebalance with old generation 0 (__consumer_offsets-19) (reason: Adding new member _confluent-ksql-default_query_CSAS_TARGET_AVRO_31-0f2d0c51-bb2b-49b7-85d7-f398d8398957-StreamThread-1-consumer-afa66494-6e48-4df5-9ab4-1cc60f04333d with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:45:44,108] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 generation 1 (__consumer_offsets-19) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:45:44,111] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 in state PreparingRebalance with old generation 1 (__consumer_offsets-19) (reason: Adding new member _confluent-ksql-default_query_CSAS_TARGET_AVRO_31-0f2d0c51-bb2b-49b7-85d7-f398d8398957-StreamThread-3-consumer-dd6ce174-0286-4119-95f7-12f76d3fa31d with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:45:44,111] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 in PreparingRebalance state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_31-0f2d0c51-bb2b-49b7-85d7-f398d8398957-StreamThread-2-consumer-bf9f6d20-2257-4999-9c51-d608c937a317 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:45:44,118] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 generation 2 (__consumer_offsets-19) with 4 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:45:44,123] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_query_CSAS_TARGET_AVRO_31-0f2d0c51-bb2b-49b7-85d7-f398d8398957-StreamThread-1-consumer-afa66494-6e48-4df5-9ab4-1cc60f04333d for group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 for generation 2. The group has 4 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:45:57,453] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_transient_transient_TARGET_AVRO_4609305502780874925_1690037157389 in Empty state. Created a new member id _confluent-ksql-default_transient_transient_TARGET_AVRO_4609305502780874925_1690037157389-003e8290-ad1a-409a-98f1-e6404898c1f7-StreamThread-1-consumer-c5876210-55ae-40f5-b431-f4a10a6e7cb7 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:45:57,455] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_transient_transient_TARGET_AVRO_4609305502780874925_1690037157389 in state PreparingRebalance with old generation 0 (__consumer_offsets-17) (reason: Adding new member _confluent-ksql-default_transient_transient_TARGET_AVRO_4609305502780874925_1690037157389-003e8290-ad1a-409a-98f1-e6404898c1f7-StreamThread-1-consumer-c5876210-55ae-40f5-b431-f4a10a6e7cb7 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:45:57,455] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_transient_transient_TARGET_AVRO_4609305502780874925_1690037157389 generation 1 (__consumer_offsets-17) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:45:57,459] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_transient_transient_TARGET_AVRO_4609305502780874925_1690037157389-003e8290-ad1a-409a-98f1-e6404898c1f7-StreamThread-1-consumer-c5876210-55ae-40f5-b431-f4a10a6e7cb7 for group _confluent-ksql-default_transient_transient_TARGET_AVRO_4609305502780874925_1690037157389 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:46:39,534] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group connect-mysql-sink in Empty state. Created a new member id connector-consumer-mysql-sink-0-54c37bea-25fb-4287-9d56-36353d0bef27 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:46:39,537] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-mysql-sink in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member connector-consumer-mysql-sink-0-54c37bea-25fb-4287-9d56-36353d0bef27 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:46:39,538] INFO [GroupCoordinator 0]: Stabilized group connect-mysql-sink generation 1 (__consumer_offsets-43) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:46:39,547] INFO [GroupCoordinator 0]: Assignment received from leader connector-consumer-mysql-sink-0-54c37bea-25fb-4287-9d56-36353d0bef27 for group connect-mysql-sink for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:46:42,906] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_transient_transient_TARGET_AVRO_4609305502780874925_1690037157389-003e8290-ad1a-409a-98f1-e6404898c1f7-StreamThread-1-consumer-c5876210-55ae-40f5-b431-f4a10a6e7cb7 in group _confluent-ksql-default_transient_transient_TARGET_AVRO_4609305502780874925_1690037157389 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:46:42,906] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_transient_transient_TARGET_AVRO_4609305502780874925_1690037157389 in state PreparingRebalance with old generation 1 (__consumer_offsets-17) (reason: removing member _confluent-ksql-default_transient_transient_TARGET_AVRO_4609305502780874925_1690037157389-003e8290-ad1a-409a-98f1-e6404898c1f7-StreamThread-1-consumer-c5876210-55ae-40f5-b431-f4a10a6e7cb7 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:46:42,906] INFO [GroupCoordinator 0]: Group _confluent-ksql-default_transient_transient_TARGET_AVRO_4609305502780874925_1690037157389 with generation 2 is now empty (__consumer_offsets-17) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:46:42,937] INFO [GroupCoordinator 0]: The following groups were deleted: _confluent-ksql-default_transient_transient_TARGET_AVRO_4609305502780874925_1690037157389. A total of 1 offsets were removed. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:46:47,839] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-mysql-sink in state PreparingRebalance with old generation 1 (__consumer_offsets-43) (reason: Removing member connector-consumer-mysql-sink-0-54c37bea-25fb-4287-9d56-36353d0bef27 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:46:47,839] INFO [GroupCoordinator 0]: Group connect-mysql-sink with generation 2 is now empty (__consumer_offsets-43) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:46:47,840] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=connector-consumer-mysql-sink-0-54c37bea-25fb-4287-9d56-36353d0bef27, groupInstanceId=None, clientId=connector-consumer-mysql-sink-0, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group connect-mysql-sink through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:48:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9244 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:48:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9803 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:48:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10709 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:48:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9180 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:48:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9693 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:48:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11379 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:48:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10620 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:48:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11209 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:48:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10742 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:48:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10502 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:48:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10891 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:48:35,575] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11371 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:48:37,882] INFO [GroupMetadataManager brokerId=0] Group connect-mysql-sink transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2023-07-22 21:48:38,729] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:51:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9334 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:51:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9949 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:51:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10918 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:51:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9258 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:51:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9766 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:51:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11589 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:51:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10706 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:51:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11284 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:51:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10830 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:51:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10655 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:51:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11174 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:51:35,572] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11371 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:52:12,684] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 17 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:52:50,552] INFO Creating topic SOURCE_DEFAULT_VALUE with configuration {cleanup.policy=delete, retention.ms=604800000} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-22 21:52:50,580] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:52:50,583] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(SOURCE_DEFAULT_VALUE-0) (kafka.server.ReplicaFetcherManager)
[2023-07-22 21:52:50,592] INFO [MergedLog partition=SOURCE_DEFAULT_VALUE-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-22 21:52:50,592] INFO Created log for partition SOURCE_DEFAULT_VALUE-0 in /tmp/kafka-logs/SOURCE_DEFAULT_VALUE-0 with properties {cleanup.policy=delete, retention.ms=604800000} (kafka.log.LogManager)
[2023-07-22 21:52:50,594] INFO [Partition SOURCE_DEFAULT_VALUE-0 broker=0] No checkpointed highwatermark is found for partition SOURCE_DEFAULT_VALUE-0 (kafka.cluster.Partition)
[2023-07-22 21:52:50,594] INFO [Partition SOURCE_DEFAULT_VALUE-0 broker=0] Log loaded for partition SOURCE_DEFAULT_VALUE-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-22 21:52:50,594] INFO Setting topicIdPartition 4uA1YVhfRUuWH-VIj8DU0Q:SOURCE_DEFAULT_VALUE-0 (kafka.tier.state.FileTierPartitionState)
[2023-07-22 21:52:50,594] INFO [MergedLog partition=SOURCE_DEFAULT_VALUE-0, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for SOURCE_DEFAULT_VALUE-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-22 21:52:50,744] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 18 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:52:51,008] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 in Empty state. Created a new member id _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35-58b51883-18d5-42bb-b7e9-55163a9d4587-StreamThread-2-consumer-49fbaaf5-1f56-462d-ba05-012261994579 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:51,010] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 in state PreparingRebalance with old generation 0 (__consumer_offsets-12) (reason: Adding new member _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35-58b51883-18d5-42bb-b7e9-55163a9d4587-StreamThread-2-consumer-49fbaaf5-1f56-462d-ba05-012261994579 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:51,011] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 generation 1 (__consumer_offsets-12) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:51,011] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 in CompletingRebalance state. Created a new member id _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35-58b51883-18d5-42bb-b7e9-55163a9d4587-StreamThread-1-consumer-4f301e11-af88-47d9-b651-0e1348c8ca74 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:51,012] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 in CompletingRebalance state. Created a new member id _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35-58b51883-18d5-42bb-b7e9-55163a9d4587-StreamThread-3-consumer-94bf26d0-1a5e-4af7-9c49-a1f16334639d and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:51,013] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 in state PreparingRebalance with old generation 1 (__consumer_offsets-12) (reason: Adding new member _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35-58b51883-18d5-42bb-b7e9-55163a9d4587-StreamThread-1-consumer-4f301e11-af88-47d9-b651-0e1348c8ca74 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:51,014] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 in PreparingRebalance state. Created a new member id _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35-58b51883-18d5-42bb-b7e9-55163a9d4587-StreamThread-4-consumer-b12b85c2-8824-4191-bb44-8d2db21556fa and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:51,015] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 generation 2 (__consumer_offsets-12) with 4 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:51,020] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35-58b51883-18d5-42bb-b7e9-55163a9d4587-StreamThread-2-consumer-49fbaaf5-1f56-462d-ba05-012261994579 for group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 for generation 2. The group has 4 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:56,179] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_TARGET_AVRO_31-0f2d0c51-bb2b-49b7-85d7-f398d8398957-StreamThread-3-consumer-dd6ce174-0286-4119-95f7-12f76d3fa31d in group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:56,179] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 in state PreparingRebalance with old generation 2 (__consumer_offsets-19) (reason: removing member _confluent-ksql-default_query_CSAS_TARGET_AVRO_31-0f2d0c51-bb2b-49b7-85d7-f398d8398957-StreamThread-3-consumer-dd6ce174-0286-4119-95f7-12f76d3fa31d on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:56,181] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_TARGET_AVRO_31-0f2d0c51-bb2b-49b7-85d7-f398d8398957-StreamThread-1-consumer-afa66494-6e48-4df5-9ab4-1cc60f04333d in group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:56,190] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_TARGET_AVRO_31-0f2d0c51-bb2b-49b7-85d7-f398d8398957-StreamThread-4-consumer-56a7eacd-f6e3-4bcd-ad3c-970cd11fedec in group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:56,194] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_TARGET_AVRO_31-0f2d0c51-bb2b-49b7-85d7-f398d8398957-StreamThread-2-consumer-bf9f6d20-2257-4999-9c51-d608c937a317 in group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:56,194] INFO [GroupCoordinator 0]: Group _confluent-ksql-default_query_CSAS_TARGET_AVRO_31 with generation 3 is now empty (__consumer_offsets-19) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:52:57,817] INFO [GroupCoordinator 0]: The following groups were deleted: _confluent-ksql-default_query_CSAS_TARGET_AVRO_31. A total of 1 offsets were removed. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:13,588] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 19 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:53:13,849] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 in Empty state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_37-c935e328-c3c6-4b29-9cf6-f26c548bf5d7-StreamThread-3-consumer-fd3c1d18-18c9-4542-9ff1-acc70c8ddb40 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:13,850] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 in state PreparingRebalance with old generation 0 (__consumer_offsets-13) (reason: Adding new member _confluent-ksql-default_query_CSAS_TARGET_AVRO_37-c935e328-c3c6-4b29-9cf6-f26c548bf5d7-StreamThread-3-consumer-fd3c1d18-18c9-4542-9ff1-acc70c8ddb40 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:13,850] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 in PreparingRebalance state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_37-c935e328-c3c6-4b29-9cf6-f26c548bf5d7-StreamThread-4-consumer-d68acbae-0726-4c2c-bfb8-01046a86e793 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:13,851] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 generation 1 (__consumer_offsets-13) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:13,852] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 in state PreparingRebalance with old generation 1 (__consumer_offsets-13) (reason: Adding new member _confluent-ksql-default_query_CSAS_TARGET_AVRO_37-c935e328-c3c6-4b29-9cf6-f26c548bf5d7-StreamThread-4-consumer-d68acbae-0726-4c2c-bfb8-01046a86e793 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:13,858] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 generation 2 (__consumer_offsets-13) with 2 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:13,859] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 in CompletingRebalance state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_37-c935e328-c3c6-4b29-9cf6-f26c548bf5d7-StreamThread-2-consumer-542e507b-96f6-4e71-bae2-703f1e94eb16 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:13,859] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 in CompletingRebalance state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_37-c935e328-c3c6-4b29-9cf6-f26c548bf5d7-StreamThread-1-consumer-6e4fb488-c68b-4154-9d42-892708f0dd8f and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:13,860] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 in state PreparingRebalance with old generation 2 (__consumer_offsets-13) (reason: Adding new member _confluent-ksql-default_query_CSAS_TARGET_AVRO_37-c935e328-c3c6-4b29-9cf6-f26c548bf5d7-StreamThread-2-consumer-542e507b-96f6-4e71-bae2-703f1e94eb16 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:13,863] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 generation 3 (__consumer_offsets-13) with 4 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:13,867] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_query_CSAS_TARGET_AVRO_37-c935e328-c3c6-4b29-9cf6-f26c548bf5d7-StreamThread-3-consumer-fd3c1d18-18c9-4542-9ff1-acc70c8ddb40 for group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 for generation 3. The group has 4 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:24,777] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_transient_transient_TARGET_AVRO_2134282686967868296_1690037604711 in Empty state. Created a new member id _confluent-ksql-default_transient_transient_TARGET_AVRO_2134282686967868296_1690037604711-5bad5f7b-398d-4176-ba68-e3efc018b39f-StreamThread-1-consumer-e38eac1b-040c-4550-bc2c-e6de2cd60b0b and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:24,778] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_transient_transient_TARGET_AVRO_2134282686967868296_1690037604711 in state PreparingRebalance with old generation 0 (__consumer_offsets-10) (reason: Adding new member _confluent-ksql-default_transient_transient_TARGET_AVRO_2134282686967868296_1690037604711-5bad5f7b-398d-4176-ba68-e3efc018b39f-StreamThread-1-consumer-e38eac1b-040c-4550-bc2c-e6de2cd60b0b with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:24,779] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_transient_transient_TARGET_AVRO_2134282686967868296_1690037604711 generation 1 (__consumer_offsets-10) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:24,784] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_transient_transient_TARGET_AVRO_2134282686967868296_1690037604711-5bad5f7b-398d-4176-ba68-e3efc018b39f-StreamThread-1-consumer-e38eac1b-040c-4550-bc2c-e6de2cd60b0b for group _confluent-ksql-default_transient_transient_TARGET_AVRO_2134282686967868296_1690037604711 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:53:39,814] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:54:10,156] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_transient_transient_TARGET_AVRO_2134282686967868296_1690037604711-5bad5f7b-398d-4176-ba68-e3efc018b39f-StreamThread-1-consumer-e38eac1b-040c-4550-bc2c-e6de2cd60b0b in group _confluent-ksql-default_transient_transient_TARGET_AVRO_2134282686967868296_1690037604711 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:54:10,157] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_transient_transient_TARGET_AVRO_2134282686967868296_1690037604711 in state PreparingRebalance with old generation 1 (__consumer_offsets-10) (reason: removing member _confluent-ksql-default_transient_transient_TARGET_AVRO_2134282686967868296_1690037604711-5bad5f7b-398d-4176-ba68-e3efc018b39f-StreamThread-1-consumer-e38eac1b-040c-4550-bc2c-e6de2cd60b0b on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:54:10,157] INFO [GroupCoordinator 0]: Group _confluent-ksql-default_transient_transient_TARGET_AVRO_2134282686967868296_1690037604711 with generation 2 is now empty (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:54:10,187] INFO [GroupCoordinator 0]: The following groups were deleted: _confluent-ksql-default_transient_transient_TARGET_AVRO_2134282686967868296_1690037604711. A total of 1 offsets were removed. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:54:35,569] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9606 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:54:35,569] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9949 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:54:35,569] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11098 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:54:35,569] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9314 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:54:35,569] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9950 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:54:35,569] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11919 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:54:35,569] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10745 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:54:35,569] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11385 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:54:35,569] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11011 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:54:35,569] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10702 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:54:35,569] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11231 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:54:35,569] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11415 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:57:35,568] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9756 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:57:35,568] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10073 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:57:35,568] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11205 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:57:35,568] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 9412 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:57:35,568] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10046 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:57:35,568] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 12048 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:57:35,568] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10988 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:57:35,568] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11585 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:57:35,568] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11062 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:57:35,568] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 10789 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:57:35,568] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11320 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:57:35,568] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-9022899771897038200] Seeking to offset 11538 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-22 21:57:37,385] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 20 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:57:55,117] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 1 and producer epoch 21 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-22 21:58:19,956] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_TARGET_AVRO_37-c935e328-c3c6-4b29-9cf6-f26c548bf5d7-StreamThread-1-consumer-6e4fb488-c68b-4154-9d42-892708f0dd8f in group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:58:19,956] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 in state PreparingRebalance with old generation 3 (__consumer_offsets-13) (reason: removing member _confluent-ksql-default_query_CSAS_TARGET_AVRO_37-c935e328-c3c6-4b29-9cf6-f26c548bf5d7-StreamThread-1-consumer-6e4fb488-c68b-4154-9d42-892708f0dd8f on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:58:19,973] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_TARGET_AVRO_37-c935e328-c3c6-4b29-9cf6-f26c548bf5d7-StreamThread-2-consumer-542e507b-96f6-4e71-bae2-703f1e94eb16 in group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:58:19,979] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_TARGET_AVRO_37-c935e328-c3c6-4b29-9cf6-f26c548bf5d7-StreamThread-3-consumer-fd3c1d18-18c9-4542-9ff1-acc70c8ddb40 in group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:58:19,980] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_TARGET_AVRO_37-c935e328-c3c6-4b29-9cf6-f26c548bf5d7-StreamThread-4-consumer-d68acbae-0726-4c2c-bfb8-01046a86e793 in group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:58:19,980] INFO [GroupCoordinator 0]: Group _confluent-ksql-default_query_CSAS_TARGET_AVRO_37 with generation 4 is now empty (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:58:22,565] INFO [GroupCoordinator 0]: The following groups were deleted: _confluent-ksql-default_query_CSAS_TARGET_AVRO_37. A total of 1 offsets were removed. (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:58:39,099] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35-58b51883-18d5-42bb-b7e9-55163a9d4587-StreamThread-4-consumer-b12b85c2-8824-4191-bb44-8d2db21556fa in group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:58:39,099] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 in state PreparingRebalance with old generation 2 (__consumer_offsets-12) (reason: removing member _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35-58b51883-18d5-42bb-b7e9-55163a9d4587-StreamThread-4-consumer-b12b85c2-8824-4191-bb44-8d2db21556fa on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:58:39,106] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35-58b51883-18d5-42bb-b7e9-55163a9d4587-StreamThread-1-consumer-4f301e11-af88-47d9-b651-0e1348c8ca74 in group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:58:39,119] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35-58b51883-18d5-42bb-b7e9-55163a9d4587-StreamThread-2-consumer-49fbaaf5-1f56-462d-ba05-012261994579 in group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:58:39,325] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35-58b51883-18d5-42bb-b7e9-55163a9d4587-StreamThread-3-consumer-94bf26d0-1a5e-4af7-9c49-a1f16334639d in group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:58:39,325] INFO [GroupCoordinator 0]: Group _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35 with generation 3 is now empty (__consumer_offsets-12) (kafka.coordinator.group.GroupCoordinator)
[2023-07-22 21:58:40,409] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-22 21:58:40,580] INFO [GroupCoordinator 0]: The following groups were deleted: _confluent-ksql-default_query_CSAS_SOURCE_DEFAULT_VALUE_35. A total of 1 offsets were removed. (kafka.coordinator.group.GroupCoordinator)
