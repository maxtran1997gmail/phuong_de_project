[2023-07-21 14:01:08,847] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1594 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:01:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1189 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:01:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1580 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:01:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1819 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:01:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1415 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:01:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1648 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:01:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1768 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:01:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1503 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:01:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1318 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:01:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1333 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:01:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1327 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:01:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1335 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:01:13,658] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:04:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1678 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:04:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1273 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:04:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1623 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:04:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1942 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:04:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1553 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:04:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1648 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:04:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1907 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:04:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1634 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:04:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1456 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:04:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1416 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:04:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1448 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:04:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1511 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:06:14,760] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:07:08,849] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1748 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:07:08,849] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1338 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:07:08,849] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1693 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:07:08,849] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2231 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:07:08,849] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1685 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:07:08,849] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1807 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:07:08,849] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1963 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:07:08,849] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1634 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:07:08,849] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1456 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:07:08,849] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1644 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:07:08,849] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1517 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:07:08,849] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1633 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:10:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1868 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:10:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1381 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:10:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1743 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:10:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2386 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:10:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1854 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:10:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1925 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:10:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2060 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:10:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1682 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:10:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1712 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:10:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1686 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:10:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1604 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:10:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1708 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:11:15,865] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:13:08,843] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2142 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:13:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1381 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:13:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1784 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:13:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2438 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:13:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1933 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:13:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1925 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:13:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2138 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:13:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1839 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:13:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1983 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:13:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1897 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:13:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1653 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:13:08,844] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1756 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:16:08,862] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2186 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:16:08,862] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1381 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:16:08,862] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1872 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:16:08,862] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2555 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:16:08,862] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2050 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:16:08,862] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2057 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:16:08,862] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2280 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:16:08,862] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1867 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:16:08,862] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2221 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:16:08,862] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2058 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:16:08,862] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1801 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:16:08,862] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1801 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:16:16,930] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:19:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2277 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:19:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1470 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:19:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1953 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:19:08,859] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2555 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:19:08,859] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2182 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:19:08,859] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2240 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:19:08,859] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2353 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:19:08,859] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2044 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:19:08,859] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2348 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:19:08,859] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2135 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:19:08,859] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1991 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:19:08,859] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1841 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:21:18,019] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:22:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2367 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:22:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1645 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:22:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2003 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:22:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2676 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:22:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2385 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:22:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2324 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:22:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2542 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:22:08,859] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2083 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:22:08,859] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2348 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:22:08,859] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2322 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:22:08,859] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2113 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:22:08,859] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1841 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:25:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2457 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:25:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1691 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:25:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2211 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:25:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2726 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:25:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2464 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:25:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2418 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:25:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2642 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:25:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2219 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:25:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2516 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:25:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2414 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:25:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2231 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:25:08,858] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1920 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:26:18,621] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:28:08,857] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2603 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:28:08,857] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1792 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:28:08,857] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2211 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:28:08,857] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2773 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:28:08,857] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2595 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:28:08,857] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2502 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:28:08,857] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2769 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:28:08,857] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2294 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:28:08,857] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2688 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:28:08,857] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2608 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:28:08,857] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2274 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:28:08,857] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2060 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:30:41,681] INFO Creating topic _confluent-ksql-default__command_topic with configuration {cleanup.policy=delete, min.insync.replicas=1, retention.ms=-1, unclean.leader.election.enable=false} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-21 14:30:41,717] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:30:41,720] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-ksql-default__command_topic-0) (kafka.server.ReplicaFetcherManager)
[2023-07-21 14:30:41,726] INFO [MergedLog partition=_confluent-ksql-default__command_topic-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:41,726] INFO Created log for partition _confluent-ksql-default__command_topic-0 in /tmp/kafka-logs/_confluent-ksql-default__command_topic-0 with properties {cleanup.policy=delete, min.insync.replicas=1, retention.ms=-1, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:41,727] INFO [Partition _confluent-ksql-default__command_topic-0 broker=0] No checkpointed highwatermark is found for partition _confluent-ksql-default__command_topic-0 (kafka.cluster.Partition)
[2023-07-21 14:30:41,727] INFO [Partition _confluent-ksql-default__command_topic-0 broker=0] Log loaded for partition _confluent-ksql-default__command_topic-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:41,727] INFO Setting topicIdPartition 1pUxEX3JSemGQTncW3VcVQ:_confluent-ksql-default__command_topic-0 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:41,727] INFO [MergedLog partition=_confluent-ksql-default__command_topic-0, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for _confluent-ksql-default__command_topic-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:41,742] INFO Creating topic default_ksql_processing_log with configuration {} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-21 14:30:41,767] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(default_ksql_processing_log-0) (kafka.server.ReplicaFetcherManager)
[2023-07-21 14:30:41,769] INFO [MergedLog partition=default_ksql_processing_log-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:41,769] INFO Created log for partition default_ksql_processing_log-0 in /tmp/kafka-logs/default_ksql_processing_log-0 with properties {} (kafka.log.LogManager)
[2023-07-21 14:30:41,770] INFO [Partition default_ksql_processing_log-0 broker=0] No checkpointed highwatermark is found for partition default_ksql_processing_log-0 (kafka.cluster.Partition)
[2023-07-21 14:30:41,770] INFO [Partition default_ksql_processing_log-0 broker=0] Log loaded for partition default_ksql_processing_log-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:41,770] INFO Setting topicIdPartition M4SYjZHzRzCeBTclZ1dieg:default_ksql_processing_log-0 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:41,770] INFO [MergedLog partition=default_ksql_processing_log-0, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for default_ksql_processing_log-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,213] INFO Creating topic __transaction_state with configuration {compression.type=uncompressed, cleanup.policy=compact, min.insync.replicas=1, segment.bytes=104857600, confluent.placement.constraints=, unclean.leader.election.enable=false} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-21 14:30:42,330] INFO Creating topic __transaction_state with configuration {compression.type=uncompressed, cleanup.policy=compact, min.insync.replicas=1, segment.bytes=104857600, confluent.placement.constraints=, unclean.leader.election.enable=false} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-21 14:30:42,364] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__transaction_state-42, __transaction_state-7, __transaction_state-13, __transaction_state-0, __transaction_state-37, __transaction_state-6, __transaction_state-32, __transaction_state-18, __transaction_state-40, __transaction_state-31, __transaction_state-45, __transaction_state-15, __transaction_state-12, __transaction_state-46, __transaction_state-48, __transaction_state-49, __transaction_state-28, __transaction_state-2, __transaction_state-20, __transaction_state-24, __transaction_state-3, __transaction_state-21, __transaction_state-29, __transaction_state-39, __transaction_state-38, __transaction_state-14, __transaction_state-10, __transaction_state-44, __transaction_state-9, __transaction_state-22, __transaction_state-43, __transaction_state-4, __transaction_state-30, __transaction_state-33, __transaction_state-25, __transaction_state-17, __transaction_state-23, __transaction_state-47, __transaction_state-26, __transaction_state-36, __transaction_state-5, __transaction_state-8, __transaction_state-16, __transaction_state-11, __transaction_state-19, __transaction_state-27, __transaction_state-41, __transaction_state-1, __transaction_state-34, __transaction_state-35) (kafka.server.ReplicaFetcherManager)
[2023-07-21 14:30:42,367] INFO [MergedLog partition=__transaction_state-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,368] INFO Created log for partition __transaction_state-3 in /tmp/kafka-logs/__transaction_state-3 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,368] INFO [Partition __transaction_state-3 broker=0] No checkpointed highwatermark is found for partition __transaction_state-3 (kafka.cluster.Partition)
[2023-07-21 14:30:42,368] INFO [Partition __transaction_state-3 broker=0] Log loaded for partition __transaction_state-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,368] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-3 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,368] INFO [MergedLog partition=__transaction_state-3, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,371] INFO [MergedLog partition=__transaction_state-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,372] INFO Created log for partition __transaction_state-18 in /tmp/kafka-logs/__transaction_state-18 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,372] INFO [Partition __transaction_state-18 broker=0] No checkpointed highwatermark is found for partition __transaction_state-18 (kafka.cluster.Partition)
[2023-07-21 14:30:42,372] INFO [Partition __transaction_state-18 broker=0] Log loaded for partition __transaction_state-18 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,372] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-18 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,372] INFO [MergedLog partition=__transaction_state-18, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,375] INFO [MergedLog partition=__transaction_state-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,376] INFO Created log for partition __transaction_state-33 in /tmp/kafka-logs/__transaction_state-33 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,376] INFO [Partition __transaction_state-33 broker=0] No checkpointed highwatermark is found for partition __transaction_state-33 (kafka.cluster.Partition)
[2023-07-21 14:30:42,376] INFO [Partition __transaction_state-33 broker=0] Log loaded for partition __transaction_state-33 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,376] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-33 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,376] INFO [MergedLog partition=__transaction_state-33, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,379] INFO [MergedLog partition=__transaction_state-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,380] INFO Created log for partition __transaction_state-11 in /tmp/kafka-logs/__transaction_state-11 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,380] INFO [Partition __transaction_state-11 broker=0] No checkpointed highwatermark is found for partition __transaction_state-11 (kafka.cluster.Partition)
[2023-07-21 14:30:42,380] INFO [Partition __transaction_state-11 broker=0] Log loaded for partition __transaction_state-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,380] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-11 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,380] INFO [MergedLog partition=__transaction_state-11, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,383] INFO [MergedLog partition=__transaction_state-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,383] INFO Created log for partition __transaction_state-26 in /tmp/kafka-logs/__transaction_state-26 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,383] INFO [Partition __transaction_state-26 broker=0] No checkpointed highwatermark is found for partition __transaction_state-26 (kafka.cluster.Partition)
[2023-07-21 14:30:42,383] INFO [Partition __transaction_state-26 broker=0] Log loaded for partition __transaction_state-26 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,384] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-26 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,384] INFO [MergedLog partition=__transaction_state-26, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,386] INFO [MergedLog partition=__transaction_state-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,387] INFO Created log for partition __transaction_state-41 in /tmp/kafka-logs/__transaction_state-41 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,387] INFO [Partition __transaction_state-41 broker=0] No checkpointed highwatermark is found for partition __transaction_state-41 (kafka.cluster.Partition)
[2023-07-21 14:30:42,387] INFO [Partition __transaction_state-41 broker=0] Log loaded for partition __transaction_state-41 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,387] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-41 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,387] INFO [MergedLog partition=__transaction_state-41, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,390] INFO [MergedLog partition=__transaction_state-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,390] INFO Created log for partition __transaction_state-4 in /tmp/kafka-logs/__transaction_state-4 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,390] INFO [Partition __transaction_state-4 broker=0] No checkpointed highwatermark is found for partition __transaction_state-4 (kafka.cluster.Partition)
[2023-07-21 14:30:42,390] INFO [Partition __transaction_state-4 broker=0] Log loaded for partition __transaction_state-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,391] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-4 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,391] INFO [MergedLog partition=__transaction_state-4, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,393] INFO [MergedLog partition=__transaction_state-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,394] INFO Created log for partition __transaction_state-19 in /tmp/kafka-logs/__transaction_state-19 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,394] INFO [Partition __transaction_state-19 broker=0] No checkpointed highwatermark is found for partition __transaction_state-19 (kafka.cluster.Partition)
[2023-07-21 14:30:42,394] INFO [Partition __transaction_state-19 broker=0] Log loaded for partition __transaction_state-19 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,394] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-19 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,394] INFO [MergedLog partition=__transaction_state-19, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,397] INFO [MergedLog partition=__transaction_state-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,397] INFO Created log for partition __transaction_state-34 in /tmp/kafka-logs/__transaction_state-34 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,397] INFO [Partition __transaction_state-34 broker=0] No checkpointed highwatermark is found for partition __transaction_state-34 (kafka.cluster.Partition)
[2023-07-21 14:30:42,398] INFO [Partition __transaction_state-34 broker=0] Log loaded for partition __transaction_state-34 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,398] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-34 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,398] INFO [MergedLog partition=__transaction_state-34, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,400] INFO [MergedLog partition=__transaction_state-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,401] INFO Created log for partition __transaction_state-49 in /tmp/kafka-logs/__transaction_state-49 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,401] INFO [Partition __transaction_state-49 broker=0] No checkpointed highwatermark is found for partition __transaction_state-49 (kafka.cluster.Partition)
[2023-07-21 14:30:42,401] INFO [Partition __transaction_state-49 broker=0] Log loaded for partition __transaction_state-49 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,401] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-49 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,401] INFO [MergedLog partition=__transaction_state-49, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,404] INFO [MergedLog partition=__transaction_state-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,404] INFO Created log for partition __transaction_state-12 in /tmp/kafka-logs/__transaction_state-12 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,404] INFO [Partition __transaction_state-12 broker=0] No checkpointed highwatermark is found for partition __transaction_state-12 (kafka.cluster.Partition)
[2023-07-21 14:30:42,404] INFO [Partition __transaction_state-12 broker=0] Log loaded for partition __transaction_state-12 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,405] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-12 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,405] INFO [MergedLog partition=__transaction_state-12, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,407] INFO [MergedLog partition=__transaction_state-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,408] INFO Created log for partition __transaction_state-27 in /tmp/kafka-logs/__transaction_state-27 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,408] INFO [Partition __transaction_state-27 broker=0] No checkpointed highwatermark is found for partition __transaction_state-27 (kafka.cluster.Partition)
[2023-07-21 14:30:42,408] INFO [Partition __transaction_state-27 broker=0] Log loaded for partition __transaction_state-27 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,408] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-27 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,408] INFO [MergedLog partition=__transaction_state-27, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,410] INFO [MergedLog partition=__transaction_state-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,411] INFO Created log for partition __transaction_state-42 in /tmp/kafka-logs/__transaction_state-42 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,411] INFO [Partition __transaction_state-42 broker=0] No checkpointed highwatermark is found for partition __transaction_state-42 (kafka.cluster.Partition)
[2023-07-21 14:30:42,411] INFO [Partition __transaction_state-42 broker=0] Log loaded for partition __transaction_state-42 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,411] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-42 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,411] INFO [MergedLog partition=__transaction_state-42, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,413] INFO [MergedLog partition=__transaction_state-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,414] INFO Created log for partition __transaction_state-24 in /tmp/kafka-logs/__transaction_state-24 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,414] INFO [Partition __transaction_state-24 broker=0] No checkpointed highwatermark is found for partition __transaction_state-24 (kafka.cluster.Partition)
[2023-07-21 14:30:42,414] INFO [Partition __transaction_state-24 broker=0] Log loaded for partition __transaction_state-24 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,414] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-24 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,415] INFO [MergedLog partition=__transaction_state-24, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,417] INFO [MergedLog partition=__transaction_state-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,418] INFO Created log for partition __transaction_state-39 in /tmp/kafka-logs/__transaction_state-39 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,418] INFO [Partition __transaction_state-39 broker=0] No checkpointed highwatermark is found for partition __transaction_state-39 (kafka.cluster.Partition)
[2023-07-21 14:30:42,418] INFO [Partition __transaction_state-39 broker=0] Log loaded for partition __transaction_state-39 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,418] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-39 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,418] INFO [MergedLog partition=__transaction_state-39, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,421] INFO [MergedLog partition=__transaction_state-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,421] INFO Created log for partition __transaction_state-1 in /tmp/kafka-logs/__transaction_state-1 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,421] INFO [Partition __transaction_state-1 broker=0] No checkpointed highwatermark is found for partition __transaction_state-1 (kafka.cluster.Partition)
[2023-07-21 14:30:42,421] INFO [Partition __transaction_state-1 broker=0] Log loaded for partition __transaction_state-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,421] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-1 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,421] INFO [MergedLog partition=__transaction_state-1, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,424] INFO [MergedLog partition=__transaction_state-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,424] INFO Created log for partition __transaction_state-32 in /tmp/kafka-logs/__transaction_state-32 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,425] INFO [Partition __transaction_state-32 broker=0] No checkpointed highwatermark is found for partition __transaction_state-32 (kafka.cluster.Partition)
[2023-07-21 14:30:42,425] INFO [Partition __transaction_state-32 broker=0] Log loaded for partition __transaction_state-32 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,425] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-32 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,425] INFO [MergedLog partition=__transaction_state-32, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,428] INFO [MergedLog partition=__transaction_state-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,428] INFO Created log for partition __transaction_state-47 in /tmp/kafka-logs/__transaction_state-47 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,428] INFO [Partition __transaction_state-47 broker=0] No checkpointed highwatermark is found for partition __transaction_state-47 (kafka.cluster.Partition)
[2023-07-21 14:30:42,428] INFO [Partition __transaction_state-47 broker=0] Log loaded for partition __transaction_state-47 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,428] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-47 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,429] INFO [MergedLog partition=__transaction_state-47, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,431] INFO [MergedLog partition=__transaction_state-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,431] INFO Created log for partition __transaction_state-9 in /tmp/kafka-logs/__transaction_state-9 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,431] INFO [Partition __transaction_state-9 broker=0] No checkpointed highwatermark is found for partition __transaction_state-9 (kafka.cluster.Partition)
[2023-07-21 14:30:42,431] INFO [Partition __transaction_state-9 broker=0] Log loaded for partition __transaction_state-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,432] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-9 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,432] INFO [MergedLog partition=__transaction_state-9, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,434] INFO [MergedLog partition=__transaction_state-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,435] INFO Created log for partition __transaction_state-40 in /tmp/kafka-logs/__transaction_state-40 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,435] INFO [Partition __transaction_state-40 broker=0] No checkpointed highwatermark is found for partition __transaction_state-40 (kafka.cluster.Partition)
[2023-07-21 14:30:42,435] INFO [Partition __transaction_state-40 broker=0] Log loaded for partition __transaction_state-40 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,435] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-40 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,435] INFO [MergedLog partition=__transaction_state-40, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,437] INFO [MergedLog partition=__transaction_state-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,438] INFO Created log for partition __transaction_state-2 in /tmp/kafka-logs/__transaction_state-2 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,438] INFO [Partition __transaction_state-2 broker=0] No checkpointed highwatermark is found for partition __transaction_state-2 (kafka.cluster.Partition)
[2023-07-21 14:30:42,438] INFO [Partition __transaction_state-2 broker=0] Log loaded for partition __transaction_state-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,438] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-2 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,439] INFO [MergedLog partition=__transaction_state-2, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,439] INFO Creating topic __transaction_state with configuration {compression.type=uncompressed, cleanup.policy=compact, min.insync.replicas=1, segment.bytes=104857600, confluent.placement.constraints=, unclean.leader.election.enable=false} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-21 14:30:42,441] INFO [MergedLog partition=__transaction_state-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,442] INFO Created log for partition __transaction_state-17 in /tmp/kafka-logs/__transaction_state-17 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,442] INFO [Partition __transaction_state-17 broker=0] No checkpointed highwatermark is found for partition __transaction_state-17 (kafka.cluster.Partition)
[2023-07-21 14:30:42,442] INFO [Partition __transaction_state-17 broker=0] Log loaded for partition __transaction_state-17 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,442] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-17 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,442] INFO [MergedLog partition=__transaction_state-17, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,445] INFO [MergedLog partition=__transaction_state-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,445] INFO Created log for partition __transaction_state-48 in /tmp/kafka-logs/__transaction_state-48 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,445] INFO [Partition __transaction_state-48 broker=0] No checkpointed highwatermark is found for partition __transaction_state-48 (kafka.cluster.Partition)
[2023-07-21 14:30:42,445] INFO [Partition __transaction_state-48 broker=0] Log loaded for partition __transaction_state-48 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,445] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-48 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,445] INFO [MergedLog partition=__transaction_state-48, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,448] INFO [MergedLog partition=__transaction_state-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,448] INFO Created log for partition __transaction_state-10 in /tmp/kafka-logs/__transaction_state-10 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,449] INFO [Partition __transaction_state-10 broker=0] No checkpointed highwatermark is found for partition __transaction_state-10 (kafka.cluster.Partition)
[2023-07-21 14:30:42,449] INFO [Partition __transaction_state-10 broker=0] Log loaded for partition __transaction_state-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,449] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-10 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,449] INFO [MergedLog partition=__transaction_state-10, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,451] INFO [MergedLog partition=__transaction_state-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,452] INFO Created log for partition __transaction_state-25 in /tmp/kafka-logs/__transaction_state-25 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,452] INFO [Partition __transaction_state-25 broker=0] No checkpointed highwatermark is found for partition __transaction_state-25 (kafka.cluster.Partition)
[2023-07-21 14:30:42,452] INFO [Partition __transaction_state-25 broker=0] Log loaded for partition __transaction_state-25 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,452] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-25 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,452] INFO [MergedLog partition=__transaction_state-25, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,454] INFO [MergedLog partition=__transaction_state-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,454] INFO Created log for partition __transaction_state-7 in /tmp/kafka-logs/__transaction_state-7 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,455] INFO [Partition __transaction_state-7 broker=0] No checkpointed highwatermark is found for partition __transaction_state-7 (kafka.cluster.Partition)
[2023-07-21 14:30:42,455] INFO [Partition __transaction_state-7 broker=0] Log loaded for partition __transaction_state-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,455] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-7 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,455] INFO [MergedLog partition=__transaction_state-7, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,457] INFO [MergedLog partition=__transaction_state-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,457] INFO Created log for partition __transaction_state-22 in /tmp/kafka-logs/__transaction_state-22 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,457] INFO [Partition __transaction_state-22 broker=0] No checkpointed highwatermark is found for partition __transaction_state-22 (kafka.cluster.Partition)
[2023-07-21 14:30:42,458] INFO [Partition __transaction_state-22 broker=0] Log loaded for partition __transaction_state-22 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,458] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-22 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,458] INFO [MergedLog partition=__transaction_state-22, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,460] INFO [MergedLog partition=__transaction_state-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,461] INFO Created log for partition __transaction_state-37 in /tmp/kafka-logs/__transaction_state-37 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,461] INFO [Partition __transaction_state-37 broker=0] No checkpointed highwatermark is found for partition __transaction_state-37 (kafka.cluster.Partition)
[2023-07-21 14:30:42,461] INFO [Partition __transaction_state-37 broker=0] Log loaded for partition __transaction_state-37 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,461] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-37 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,461] INFO [MergedLog partition=__transaction_state-37, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,463] INFO [MergedLog partition=__transaction_state-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,464] INFO Created log for partition __transaction_state-0 in /tmp/kafka-logs/__transaction_state-0 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,464] INFO [Partition __transaction_state-0 broker=0] No checkpointed highwatermark is found for partition __transaction_state-0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,464] INFO [Partition __transaction_state-0 broker=0] Log loaded for partition __transaction_state-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,464] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-0 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,464] INFO [MergedLog partition=__transaction_state-0, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,466] INFO [MergedLog partition=__transaction_state-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,467] INFO Created log for partition __transaction_state-15 in /tmp/kafka-logs/__transaction_state-15 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,467] INFO [Partition __transaction_state-15 broker=0] No checkpointed highwatermark is found for partition __transaction_state-15 (kafka.cluster.Partition)
[2023-07-21 14:30:42,467] INFO [Partition __transaction_state-15 broker=0] Log loaded for partition __transaction_state-15 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,467] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-15 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,467] INFO [MergedLog partition=__transaction_state-15, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,469] INFO [MergedLog partition=__transaction_state-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,470] INFO Created log for partition __transaction_state-30 in /tmp/kafka-logs/__transaction_state-30 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,470] INFO [Partition __transaction_state-30 broker=0] No checkpointed highwatermark is found for partition __transaction_state-30 (kafka.cluster.Partition)
[2023-07-21 14:30:42,470] INFO [Partition __transaction_state-30 broker=0] Log loaded for partition __transaction_state-30 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,470] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-30 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,470] INFO [MergedLog partition=__transaction_state-30, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,472] INFO [MergedLog partition=__transaction_state-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,472] INFO Created log for partition __transaction_state-45 in /tmp/kafka-logs/__transaction_state-45 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,472] INFO [Partition __transaction_state-45 broker=0] No checkpointed highwatermark is found for partition __transaction_state-45 (kafka.cluster.Partition)
[2023-07-21 14:30:42,473] INFO [Partition __transaction_state-45 broker=0] Log loaded for partition __transaction_state-45 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,473] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-45 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,473] INFO [MergedLog partition=__transaction_state-45, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,475] INFO [MergedLog partition=__transaction_state-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,475] INFO Created log for partition __transaction_state-8 in /tmp/kafka-logs/__transaction_state-8 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,475] INFO [Partition __transaction_state-8 broker=0] No checkpointed highwatermark is found for partition __transaction_state-8 (kafka.cluster.Partition)
[2023-07-21 14:30:42,475] INFO [Partition __transaction_state-8 broker=0] Log loaded for partition __transaction_state-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,475] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-8 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,475] INFO [MergedLog partition=__transaction_state-8, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,477] INFO [MergedLog partition=__transaction_state-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,478] INFO Created log for partition __transaction_state-23 in /tmp/kafka-logs/__transaction_state-23 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,478] INFO [Partition __transaction_state-23 broker=0] No checkpointed highwatermark is found for partition __transaction_state-23 (kafka.cluster.Partition)
[2023-07-21 14:30:42,478] INFO [Partition __transaction_state-23 broker=0] Log loaded for partition __transaction_state-23 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,478] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-23 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,478] INFO [MergedLog partition=__transaction_state-23, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,480] INFO [MergedLog partition=__transaction_state-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,481] INFO Created log for partition __transaction_state-38 in /tmp/kafka-logs/__transaction_state-38 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,481] INFO [Partition __transaction_state-38 broker=0] No checkpointed highwatermark is found for partition __transaction_state-38 (kafka.cluster.Partition)
[2023-07-21 14:30:42,481] INFO [Partition __transaction_state-38 broker=0] Log loaded for partition __transaction_state-38 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,481] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-38 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,481] INFO [MergedLog partition=__transaction_state-38, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,484] INFO [MergedLog partition=__transaction_state-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,484] INFO Created log for partition __transaction_state-16 in /tmp/kafka-logs/__transaction_state-16 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,484] INFO [Partition __transaction_state-16 broker=0] No checkpointed highwatermark is found for partition __transaction_state-16 (kafka.cluster.Partition)
[2023-07-21 14:30:42,484] INFO [Partition __transaction_state-16 broker=0] Log loaded for partition __transaction_state-16 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,484] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-16 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,485] INFO [MergedLog partition=__transaction_state-16, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,487] INFO [MergedLog partition=__transaction_state-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,487] INFO Created log for partition __transaction_state-31 in /tmp/kafka-logs/__transaction_state-31 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,487] INFO [Partition __transaction_state-31 broker=0] No checkpointed highwatermark is found for partition __transaction_state-31 (kafka.cluster.Partition)
[2023-07-21 14:30:42,487] INFO [Partition __transaction_state-31 broker=0] Log loaded for partition __transaction_state-31 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,487] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-31 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,488] INFO [MergedLog partition=__transaction_state-31, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,489] INFO [MergedLog partition=__transaction_state-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,490] INFO Created log for partition __transaction_state-46 in /tmp/kafka-logs/__transaction_state-46 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,490] INFO [Partition __transaction_state-46 broker=0] No checkpointed highwatermark is found for partition __transaction_state-46 (kafka.cluster.Partition)
[2023-07-21 14:30:42,490] INFO [Partition __transaction_state-46 broker=0] Log loaded for partition __transaction_state-46 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,490] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-46 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,490] INFO [MergedLog partition=__transaction_state-46, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,492] INFO [MergedLog partition=__transaction_state-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,492] INFO Created log for partition __transaction_state-5 in /tmp/kafka-logs/__transaction_state-5 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,492] INFO [Partition __transaction_state-5 broker=0] No checkpointed highwatermark is found for partition __transaction_state-5 (kafka.cluster.Partition)
[2023-07-21 14:30:42,492] INFO [Partition __transaction_state-5 broker=0] Log loaded for partition __transaction_state-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,492] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-5 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,493] INFO [MergedLog partition=__transaction_state-5, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,495] INFO [MergedLog partition=__transaction_state-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,495] INFO Created log for partition __transaction_state-20 in /tmp/kafka-logs/__transaction_state-20 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,495] INFO [Partition __transaction_state-20 broker=0] No checkpointed highwatermark is found for partition __transaction_state-20 (kafka.cluster.Partition)
[2023-07-21 14:30:42,495] INFO [Partition __transaction_state-20 broker=0] Log loaded for partition __transaction_state-20 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,496] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-20 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,496] INFO [MergedLog partition=__transaction_state-20, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,498] INFO [MergedLog partition=__transaction_state-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,499] INFO Created log for partition __transaction_state-35 in /tmp/kafka-logs/__transaction_state-35 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,499] INFO [Partition __transaction_state-35 broker=0] No checkpointed highwatermark is found for partition __transaction_state-35 (kafka.cluster.Partition)
[2023-07-21 14:30:42,499] INFO [Partition __transaction_state-35 broker=0] Log loaded for partition __transaction_state-35 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,499] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-35 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,499] INFO [MergedLog partition=__transaction_state-35, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,501] INFO [MergedLog partition=__transaction_state-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,502] INFO Created log for partition __transaction_state-13 in /tmp/kafka-logs/__transaction_state-13 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,502] INFO [Partition __transaction_state-13 broker=0] No checkpointed highwatermark is found for partition __transaction_state-13 (kafka.cluster.Partition)
[2023-07-21 14:30:42,502] INFO [Partition __transaction_state-13 broker=0] Log loaded for partition __transaction_state-13 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,502] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-13 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,502] INFO [MergedLog partition=__transaction_state-13, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,505] INFO [MergedLog partition=__transaction_state-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,505] INFO Created log for partition __transaction_state-28 in /tmp/kafka-logs/__transaction_state-28 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,505] INFO [Partition __transaction_state-28 broker=0] No checkpointed highwatermark is found for partition __transaction_state-28 (kafka.cluster.Partition)
[2023-07-21 14:30:42,505] INFO [Partition __transaction_state-28 broker=0] Log loaded for partition __transaction_state-28 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,506] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-28 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,506] INFO [MergedLog partition=__transaction_state-28, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,508] INFO [MergedLog partition=__transaction_state-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,509] INFO Created log for partition __transaction_state-43 in /tmp/kafka-logs/__transaction_state-43 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,509] INFO [Partition __transaction_state-43 broker=0] No checkpointed highwatermark is found for partition __transaction_state-43 (kafka.cluster.Partition)
[2023-07-21 14:30:42,509] INFO [Partition __transaction_state-43 broker=0] Log loaded for partition __transaction_state-43 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,509] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-43 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,509] INFO [MergedLog partition=__transaction_state-43, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,511] INFO [MergedLog partition=__transaction_state-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,512] INFO Created log for partition __transaction_state-6 in /tmp/kafka-logs/__transaction_state-6 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,512] INFO [Partition __transaction_state-6 broker=0] No checkpointed highwatermark is found for partition __transaction_state-6 (kafka.cluster.Partition)
[2023-07-21 14:30:42,512] INFO [Partition __transaction_state-6 broker=0] Log loaded for partition __transaction_state-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,512] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-6 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,512] INFO [MergedLog partition=__transaction_state-6, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,514] INFO [MergedLog partition=__transaction_state-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,515] INFO Created log for partition __transaction_state-21 in /tmp/kafka-logs/__transaction_state-21 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,515] INFO [Partition __transaction_state-21 broker=0] No checkpointed highwatermark is found for partition __transaction_state-21 (kafka.cluster.Partition)
[2023-07-21 14:30:42,515] INFO [Partition __transaction_state-21 broker=0] Log loaded for partition __transaction_state-21 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,515] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-21 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,515] INFO [MergedLog partition=__transaction_state-21, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,517] INFO [MergedLog partition=__transaction_state-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,518] INFO Created log for partition __transaction_state-36 in /tmp/kafka-logs/__transaction_state-36 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,518] INFO [Partition __transaction_state-36 broker=0] No checkpointed highwatermark is found for partition __transaction_state-36 (kafka.cluster.Partition)
[2023-07-21 14:30:42,518] INFO [Partition __transaction_state-36 broker=0] Log loaded for partition __transaction_state-36 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,518] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-36 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,518] INFO [MergedLog partition=__transaction_state-36, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,520] INFO [MergedLog partition=__transaction_state-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,520] INFO Created log for partition __transaction_state-14 in /tmp/kafka-logs/__transaction_state-14 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,521] INFO [Partition __transaction_state-14 broker=0] No checkpointed highwatermark is found for partition __transaction_state-14 (kafka.cluster.Partition)
[2023-07-21 14:30:42,521] INFO [Partition __transaction_state-14 broker=0] Log loaded for partition __transaction_state-14 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,521] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-14 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,521] INFO [MergedLog partition=__transaction_state-14, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,523] INFO [MergedLog partition=__transaction_state-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,523] INFO Created log for partition __transaction_state-29 in /tmp/kafka-logs/__transaction_state-29 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,523] INFO [Partition __transaction_state-29 broker=0] No checkpointed highwatermark is found for partition __transaction_state-29 (kafka.cluster.Partition)
[2023-07-21 14:30:42,523] INFO [Partition __transaction_state-29 broker=0] Log loaded for partition __transaction_state-29 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,523] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-29 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,524] INFO [MergedLog partition=__transaction_state-29, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,526] INFO [MergedLog partition=__transaction_state-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:30:42,526] INFO Created log for partition __transaction_state-44 in /tmp/kafka-logs/__transaction_state-44 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-07-21 14:30:42,526] INFO [Partition __transaction_state-44 broker=0] No checkpointed highwatermark is found for partition __transaction_state-44 (kafka.cluster.Partition)
[2023-07-21 14:30:42,526] INFO [Partition __transaction_state-44 broker=0] Log loaded for partition __transaction_state-44 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:30:42,526] INFO Setting topicIdPartition C56fWRN8T8CrvNGJEAC9gg:__transaction_state-44 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:30:42,527] INFO [MergedLog partition=__transaction_state-44, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for __transaction_state-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:30:42,528] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 3 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,533] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 18 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 33 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 11 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 26 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 41 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 4 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 19 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 34 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 49 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 12 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 27 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 42 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 24 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-3 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 39 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 1 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 32 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 47 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 9 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,534] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 40 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 2 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 17 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 48 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 10 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 25 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 7 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 22 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 37 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 0 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 15 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 30 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 45 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 8 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 23 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,535] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 38 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 16 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 31 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 46 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 5 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 20 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 35 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 13 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 28 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 43 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 6 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 21 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 36 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 14 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 29 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,536] INFO [TransactionCoordinator id=0] Elected as the txn coordinator for partition 44 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:30:42,540] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-3 in 5 milliseconds, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,544] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-3 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,544] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-18 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,545] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-18 in 11 milliseconds, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,546] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-18 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,546] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-33 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,547] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-33 in 13 milliseconds, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,547] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-33 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,547] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-11 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,548] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-11 in 14 milliseconds, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,548] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-11 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,548] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-26 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,550] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-26 in 16 milliseconds, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,550] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-26 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,550] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-41 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,551] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-41 in 17 milliseconds, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,551] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-41 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,551] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-4 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,552] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-4 in 18 milliseconds, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,552] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-4 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,552] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-19 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,553] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-19 in 19 milliseconds, of which 18 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,553] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-19 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,553] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-34 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,555] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-34 in 21 milliseconds, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,555] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-34 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,555] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-49 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,556] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-49 in 22 milliseconds, of which 21 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,556] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-49 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,556] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-12 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,557] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-12 in 23 milliseconds, of which 22 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,557] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-12 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,557] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-27 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,558] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-27 in 24 milliseconds, of which 23 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,558] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-27 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,558] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-42 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,559] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-42 in 25 milliseconds, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,559] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-42 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,559] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-24 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,560] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-24 in 26 milliseconds, of which 25 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,560] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-24 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,560] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-39 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,562] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-39 in 28 milliseconds, of which 26 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,562] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-39 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,562] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-1 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,563] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-1 in 29 milliseconds, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,563] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-1 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,563] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-32 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,564] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-32 in 30 milliseconds, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,564] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-32 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,564] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-47 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,565] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-47 in 31 milliseconds, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,565] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-47 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,565] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-9 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,566] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-9 in 32 milliseconds, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,566] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-9 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,566] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-40 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,567] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-40 in 32 milliseconds, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,568] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-40 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,568] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-2 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,569] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-2 in 34 milliseconds, of which 33 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,569] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-2 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,569] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-17 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,570] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-17 in 35 milliseconds, of which 34 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,570] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-17 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,570] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-48 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,571] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-48 in 36 milliseconds, of which 35 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,571] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-48 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,571] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-10 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,572] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-10 in 37 milliseconds, of which 36 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,572] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-10 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,572] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-25 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,573] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-25 in 38 milliseconds, of which 37 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,573] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-25 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,573] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-7 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,574] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-7 in 39 milliseconds, of which 38 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,574] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-7 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,574] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-22 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,575] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-22 in 40 milliseconds, of which 39 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,575] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-22 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,576] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-37 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,577] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-37 in 42 milliseconds, of which 41 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,577] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-37 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,577] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-0 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,578] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-0 in 43 milliseconds, of which 42 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,578] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-0 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,578] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-15 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,579] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-15 in 44 milliseconds, of which 43 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,579] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-15 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,579] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-30 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,580] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-30 in 45 milliseconds, of which 44 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,580] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-30 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,580] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-45 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,580] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-45 in 45 milliseconds, of which 45 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,581] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-45 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,581] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-8 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,581] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-8 in 46 milliseconds, of which 46 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,581] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-8 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,581] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-23 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,582] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-23 in 47 milliseconds, of which 46 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,582] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-23 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,582] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-38 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,583] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-38 in 47 milliseconds, of which 46 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,583] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-38 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,583] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-16 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,584] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-16 in 48 milliseconds, of which 47 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,584] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-16 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,584] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-31 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,585] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-31 in 49 milliseconds, of which 48 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,585] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-31 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,585] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-46 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,586] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-46 in 50 milliseconds, of which 49 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,586] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-46 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,586] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-5 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,587] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-5 in 51 milliseconds, of which 50 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,587] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-5 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,587] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-20 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,588] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-20 in 52 milliseconds, of which 51 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,588] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-20 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,588] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-35 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,589] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-35 in 53 milliseconds, of which 52 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,589] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-35 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,589] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-13 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,590] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-13 in 54 milliseconds, of which 53 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,590] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-13 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,590] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-28 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,591] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-28 in 55 milliseconds, of which 54 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,591] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-28 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,591] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-43 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,592] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-43 in 56 milliseconds, of which 55 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,592] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-43 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,592] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-6 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,594] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-6 in 58 milliseconds, of which 56 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,594] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-6 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,594] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-21 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,595] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-21 in 59 milliseconds, of which 58 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,595] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-21 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,595] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-36 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,596] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-36 in 60 milliseconds, of which 59 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,596] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-36 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,596] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-14 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,597] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-14 in 61 milliseconds, of which 60 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,597] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-14 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,597] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-29 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,598] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-29 in 62 milliseconds, of which 61 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,598] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-29 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,598] INFO [Transaction State Manager 0]: Loading transaction metadata from __transaction_state-44 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,599] INFO [Transaction State Manager 0]: Finished loading 0 transaction metadata from __transaction_state-44 in 63 milliseconds, of which 62 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,599] INFO [Transaction State Manager 0]: Completed loading transaction metadata from __transaction_state-44 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-07-21 14:30:42,671] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 2 and producer epoch 0 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:31:08,856] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2643 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:31:08,856] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1834 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:31:08,856] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2257 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:31:08,856] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2836 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:31:08,856] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2870 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:31:08,856] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2557 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:31:08,856] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2863 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:31:08,856] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2350 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:31:08,856] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2977 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:31:08,856] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2658 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:31:08,856] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2320 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:31:08,856] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2264 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:31:19,220] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:34:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2702 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:34:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 1878 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:34:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2441 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:34:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2873 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:34:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3123 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:34:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2623 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:34:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3053 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:34:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2400 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:34:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3034 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:34:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2703 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:34:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2570 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:34:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2352 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:36:19,725] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:36:29,601] INFO [GroupCoordinator 0]: Preparing to rebalance group console-consumer-96031 in state PreparingRebalance with old generation 1 (__consumer_offsets-20) (reason: Removing member console-consumer-96667608-f550-4e8e-87d9-b387258bba7b on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:36:29,602] INFO [GroupCoordinator 0]: Group console-consumer-96031 with generation 2 is now empty (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:36:29,603] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=console-consumer-96667608-f550-4e8e-87d9-b387258bba7b, groupInstanceId=None, clientId=console-consumer, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group console-consumer-96031 through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:36:46,623] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 2 and producer epoch 1 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:37:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2836 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:37:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2034 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:37:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2556 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:37:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2911 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:37:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3320 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:37:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2623 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:37:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3104 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:37:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2486 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:37:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3117 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:37:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2810 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:37:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2826 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:37:08,855] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2578 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:38:45,084] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_transient_transient_SOURCE_77194701767595839_1689925124746 in Empty state. Created a new member id _confluent-ksql-default_transient_transient_SOURCE_77194701767595839_1689925124746-4fb567cd-dcad-4760-8dff-8608e12ecf56-StreamThread-1-consumer-e9dab80b-8431-4ed5-996b-0bdc77e7dd6e and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:38:45,088] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_transient_transient_SOURCE_77194701767595839_1689925124746 in state PreparingRebalance with old generation 0 (__consumer_offsets-14) (reason: Adding new member _confluent-ksql-default_transient_transient_SOURCE_77194701767595839_1689925124746-4fb567cd-dcad-4760-8dff-8608e12ecf56-StreamThread-1-consumer-e9dab80b-8431-4ed5-996b-0bdc77e7dd6e with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:38:45,090] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_transient_transient_SOURCE_77194701767595839_1689925124746 generation 1 (__consumer_offsets-14) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:38:45,143] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_transient_transient_SOURCE_77194701767595839_1689925124746-4fb567cd-dcad-4760-8dff-8608e12ecf56-StreamThread-1-consumer-e9dab80b-8431-4ed5-996b-0bdc77e7dd6e for group _confluent-ksql-default_transient_transient_SOURCE_77194701767595839_1689925124746 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:39:30,498] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_transient_transient_SOURCE_77194701767595839_1689925124746-4fb567cd-dcad-4760-8dff-8608e12ecf56-StreamThread-1-consumer-e9dab80b-8431-4ed5-996b-0bdc77e7dd6e in group _confluent-ksql-default_transient_transient_SOURCE_77194701767595839_1689925124746 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:39:30,499] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_transient_transient_SOURCE_77194701767595839_1689925124746 in state PreparingRebalance with old generation 1 (__consumer_offsets-14) (reason: removing member _confluent-ksql-default_transient_transient_SOURCE_77194701767595839_1689925124746-4fb567cd-dcad-4760-8dff-8608e12ecf56-StreamThread-1-consumer-e9dab80b-8431-4ed5-996b-0bdc77e7dd6e on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:39:30,499] INFO [GroupCoordinator 0]: Group _confluent-ksql-default_transient_transient_SOURCE_77194701767595839_1689925124746 with generation 2 is now empty (__consumer_offsets-14) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:39:30,579] INFO [GroupCoordinator 0]: The following groups were deleted: _confluent-ksql-default_transient_transient_SOURCE_77194701767595839_1689925124746. A total of 1 offsets were removed. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:40:08,853] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2953 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:40:08,853] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2096 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:40:08,854] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2653 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:40:08,854] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3154 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:40:08,854] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3507 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:40:08,854] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2828 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:40:08,854] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3276 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:40:08,854] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2569 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:40:08,854] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3244 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:40:08,854] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2810 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:40:08,854] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2942 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:40:08,854] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2618 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:41:20,824] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:41:35,512] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_transient_transient_SOURCE_4827440445596780438_1689925295372 in Empty state. Created a new member id _confluent-ksql-default_transient_transient_SOURCE_4827440445596780438_1689925295372-65ac9652-65a7-4fbc-82fd-41b2d915e5f1-StreamThread-1-consumer-e07f503f-dbf4-4b24-876e-9245497e9042 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:41:35,513] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_transient_transient_SOURCE_4827440445596780438_1689925295372 in state PreparingRebalance with old generation 0 (__consumer_offsets-17) (reason: Adding new member _confluent-ksql-default_transient_transient_SOURCE_4827440445596780438_1689925295372-65ac9652-65a7-4fbc-82fd-41b2d915e5f1-StreamThread-1-consumer-e07f503f-dbf4-4b24-876e-9245497e9042 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:41:35,514] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_transient_transient_SOURCE_4827440445596780438_1689925295372 generation 1 (__consumer_offsets-17) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:41:35,519] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_transient_transient_SOURCE_4827440445596780438_1689925295372-65ac9652-65a7-4fbc-82fd-41b2d915e5f1-StreamThread-1-consumer-e07f503f-dbf4-4b24-876e-9245497e9042 for group _confluent-ksql-default_transient_transient_SOURCE_4827440445596780438_1689925295372 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:41:48,066] INFO Creating topic mysql_users_avro with configuration {cleanup.policy=delete, retention.ms=604800000} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-21 14:41:48,085] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:41:48,086] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(mysql_users_avro-0) (kafka.server.ReplicaFetcherManager)
[2023-07-21 14:41:48,089] INFO [MergedLog partition=mysql_users_avro-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:41:48,089] INFO Created log for partition mysql_users_avro-0 in /tmp/kafka-logs/mysql_users_avro-0 with properties {cleanup.policy=delete, retention.ms=604800000} (kafka.log.LogManager)
[2023-07-21 14:41:48,090] INFO [Partition mysql_users_avro-0 broker=0] No checkpointed highwatermark is found for partition mysql_users_avro-0 (kafka.cluster.Partition)
[2023-07-21 14:41:48,090] INFO [Partition mysql_users_avro-0 broker=0] Log loaded for partition mysql_users_avro-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:41:48,090] INFO Setting topicIdPartition BmBNd_QQRzO569Nv8vDj6w:mysql_users_avro-0 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:41:48,090] INFO [MergedLog partition=mysql_users_avro-0, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for mysql_users_avro-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:42:20,779] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_transient_transient_SOURCE_4827440445596780438_1689925295372-65ac9652-65a7-4fbc-82fd-41b2d915e5f1-StreamThread-1-consumer-e07f503f-dbf4-4b24-876e-9245497e9042 in group _confluent-ksql-default_transient_transient_SOURCE_4827440445596780438_1689925295372 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:42:20,779] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_transient_transient_SOURCE_4827440445596780438_1689925295372 in state PreparingRebalance with old generation 1 (__consumer_offsets-17) (reason: removing member _confluent-ksql-default_transient_transient_SOURCE_4827440445596780438_1689925295372-65ac9652-65a7-4fbc-82fd-41b2d915e5f1-StreamThread-1-consumer-e07f503f-dbf4-4b24-876e-9245497e9042 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:42:20,779] INFO [GroupCoordinator 0]: Group _confluent-ksql-default_transient_transient_SOURCE_4827440445596780438_1689925295372 with generation 2 is now empty (__consumer_offsets-17) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:42:20,834] INFO [GroupCoordinator 0]: The following groups were deleted: _confluent-ksql-default_transient_transient_SOURCE_4827440445596780438_1689925295372. A total of 1 offsets were removed. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:43:08,851] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2994 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:43:08,851] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2274 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:43:08,851] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2739 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:43:08,851] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3237 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:43:08,851] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3557 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:43:08,851] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3005 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:43:08,851] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3384 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:43:08,851] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2827 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:43:08,851] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3293 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:43:08,851] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2951 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:43:08,851] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3069 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:43:08,851] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2773 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:45:02,020] INFO [GroupMetadataManager brokerId=0] Group console-consumer-96031 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2023-07-21 14:46:08,850] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3295 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:46:08,850] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2421 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:46:08,850] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2782 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:46:08,850] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3421 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:46:08,850] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3603 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:46:08,850] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3133 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:46:08,850] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3478 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:46:08,850] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2977 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:46:08,850] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3480 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:46:08,850] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3037 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:46:08,850] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3109 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:46:08,850] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2824 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:46:21,384] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:46:46,841] INFO [TransactionCoordinator id=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:47:06,344] INFO Creating topic BTC_Avro with configuration {} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-21 14:47:06,365] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(BTC_Avro-0) (kafka.server.ReplicaFetcherManager)
[2023-07-21 14:47:06,368] INFO [MergedLog partition=BTC_Avro-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:47:06,369] INFO Created log for partition BTC_Avro-0 in /tmp/kafka-logs/BTC_Avro-0 with properties {} (kafka.log.LogManager)
[2023-07-21 14:47:06,370] INFO [Partition BTC_Avro-0 broker=0] No checkpointed highwatermark is found for partition BTC_Avro-0 (kafka.cluster.Partition)
[2023-07-21 14:47:06,370] INFO [Partition BTC_Avro-0 broker=0] Log loaded for partition BTC_Avro-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:47:06,370] INFO Setting topicIdPartition 8_fJYn5qQdywO8wKAbKBYg:BTC_Avro-0 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:47:06,370] INFO [MergedLog partition=BTC_Avro-0, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for BTC_Avro-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:49:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3333 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:49:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2524 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:49:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2891 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:49:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3524 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:49:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3766 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:49:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3285 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:49:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3579 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:49:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3240 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:49:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3597 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:49:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3203 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:49:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3195 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:49:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2881 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:50:42,338] INFO Creating topic _schemas with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-21 14:50:42,361] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2023-07-21 14:50:42,364] INFO [MergedLog partition=_schemas-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:50:42,365] INFO Created log for partition _schemas-0 in /tmp/kafka-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-07-21 14:50:42,366] INFO [Partition _schemas-0 broker=0] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2023-07-21 14:50:42,366] INFO [Partition _schemas-0 broker=0] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:50:42,367] INFO Setting topicIdPartition eX58ZXE4SCmzrKr2T7mGxw:_schemas-0 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:50:42,367] INFO [MergedLog partition=_schemas-0, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for _schemas-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:50:42,622] INFO Creating topic _schema_encoders with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2023-07-21 14:50:42,643] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_schema_encoders-0) (kafka.server.ReplicaFetcherManager)
[2023-07-21 14:50:42,646] INFO [MergedLog partition=_schema_encoders-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-07-21 14:50:42,646] INFO Created log for partition _schema_encoders-0 in /tmp/kafka-logs/_schema_encoders-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-07-21 14:50:42,647] INFO [Partition _schema_encoders-0 broker=0] No checkpointed highwatermark is found for partition _schema_encoders-0 (kafka.cluster.Partition)
[2023-07-21 14:50:42,647] INFO [Partition _schema_encoders-0 broker=0] Log loaded for partition _schema_encoders-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-07-21 14:50:42,647] INFO Setting topicIdPartition cxz51R1kRqSAaaO8yaRI9Q:_schema_encoders-0 (kafka.tier.state.FileTierPartitionState)
[2023-07-21 14:50:42,647] INFO [MergedLog partition=_schema_encoders-0, dir=/tmp/kafka-logs] Initializing tier metadata without recovery for _schema_encoders-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-07-21 14:50:42,704] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-6fa966ab-6e98-4e7a-83e5-3df96b32b726 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:50:42,707] INFO [GroupCoordinator 0]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-6fa966ab-6e98-4e7a-83e5-3df96b32b726 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:50:42,707] INFO [GroupCoordinator 0]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:50:42,720] INFO [GroupCoordinator 0]: Assignment received from leader sr-1-6fa966ab-6e98-4e7a-83e5-3df96b32b726 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:51:02,321] INFO [TransactionCoordinator id=0] Initialized transactionalId default_ with producerId 2 and producer epoch 2 on partition __transaction_state-44 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-07-21 14:51:02,651] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in Empty state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-dcc69869-6302-40f8-82be-ed745cb81d62-StreamThread-3-consumer-580fde90-33d9-4214-8584-10beb899d7de and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:51:02,652] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in Empty state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-dcc69869-6302-40f8-82be-ed745cb81d62-StreamThread-2-consumer-951ec2cd-c6f4-4b5f-9328-2d5340b94dbf and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:51:02,652] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in Empty state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-dcc69869-6302-40f8-82be-ed745cb81d62-StreamThread-1-consumer-01e47a82-47f6-4a20-af64-2d337f492580 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:51:02,653] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in state PreparingRebalance with old generation 0 (__consumer_offsets-20) (reason: Adding new member _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-dcc69869-6302-40f8-82be-ed745cb81d62-StreamThread-3-consumer-580fde90-33d9-4214-8584-10beb899d7de with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:51:02,654] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 generation 1 (__consumer_offsets-20) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:51:02,657] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in state PreparingRebalance with old generation 1 (__consumer_offsets-20) (reason: Adding new member _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-dcc69869-6302-40f8-82be-ed745cb81d62-StreamThread-1-consumer-01e47a82-47f6-4a20-af64-2d337f492580 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:51:02,658] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 in PreparingRebalance state. Created a new member id _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-dcc69869-6302-40f8-82be-ed745cb81d62-StreamThread-4-consumer-d1ce6952-4edf-4938-92e4-accf50a4c9e0 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:51:02,663] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 generation 2 (__consumer_offsets-20) with 4 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:51:02,674] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_query_CSAS_TARGET_AVRO_3-dcc69869-6302-40f8-82be-ed745cb81d62-StreamThread-3-consumer-580fde90-33d9-4214-8584-10beb899d7de for group _confluent-ksql-default_query_CSAS_TARGET_AVRO_3 for generation 2. The group has 4 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:51:22,485] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:52:08,847] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3394 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:52:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2633 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:52:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2891 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:52:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3620 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:52:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3973 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:52:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3580 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:52:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3634 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:52:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3339 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:52:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3791 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:52:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3396 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:52:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3241 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:52:08,848] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2986 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:54:16,595] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group console-consumer-87426 in Empty state. Created a new member id console-consumer-6c23ed2b-7f0d-4f75-ac1b-f0eb723903ca and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:54:16,597] INFO [GroupCoordinator 0]: Preparing to rebalance group console-consumer-87426 in state PreparingRebalance with old generation 0 (__consumer_offsets-8) (reason: Adding new member console-consumer-6c23ed2b-7f0d-4f75-ac1b-f0eb723903ca with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:54:16,598] INFO [GroupCoordinator 0]: Stabilized group console-consumer-87426 generation 1 (__consumer_offsets-8) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:54:16,606] INFO [GroupCoordinator 0]: Assignment received from leader console-consumer-6c23ed2b-7f0d-4f75-ac1b-f0eb723903ca for group console-consumer-87426 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:55:03,109] INFO [GroupCoordinator 0]: Preparing to rebalance group console-consumer-87426 in state PreparingRebalance with old generation 1 (__consumer_offsets-8) (reason: Removing member console-consumer-6c23ed2b-7f0d-4f75-ac1b-f0eb723903ca on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:55:03,109] INFO [GroupCoordinator 0]: Group console-consumer-87426 with generation 2 is now empty (__consumer_offsets-8) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:55:03,109] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=console-consumer-6c23ed2b-7f0d-4f75-ac1b-f0eb723903ca, groupInstanceId=None, clientId=console-consumer, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group console-consumer-87426 through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:55:08,846] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3524 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:55:08,846] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2676 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:55:08,846] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2996 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:55:08,846] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3757 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:55:08,846] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 4146 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:55:08,846] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3711 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:55:08,846] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3842 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:55:08,846] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3404 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:55:08,846] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3949 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:55:08,846] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3467 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:55:08,846] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3421 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:55:08,846] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3064 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:55:11,170] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group console-consumer-59858 in Empty state. Created a new member id console-consumer-4c623fd8-3e30-4869-b064-736786f7cc49 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:55:11,172] INFO [GroupCoordinator 0]: Preparing to rebalance group console-consumer-59858 in state PreparingRebalance with old generation 0 (__consumer_offsets-16) (reason: Adding new member console-consumer-4c623fd8-3e30-4869-b064-736786f7cc49 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:55:11,173] INFO [GroupCoordinator 0]: Stabilized group console-consumer-59858 generation 1 (__consumer_offsets-16) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:55:11,177] INFO [GroupCoordinator 0]: Assignment received from leader console-consumer-4c623fd8-3e30-4869-b064-736786f7cc49 for group console-consumer-59858 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:56:23,584] INFO [AdminClient clientId=null-admin-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)
[2023-07-21 14:57:27,001] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_transient_transient_TARGET_AVRO_7080193500955133055_1689926246950 in Empty state. Created a new member id _confluent-ksql-default_transient_transient_TARGET_AVRO_7080193500955133055_1689926246950-42bb0896-dfa1-4ac5-8735-71f3b753f050-StreamThread-1-consumer-27ba145b-41ba-4fec-9d37-5873e747766f and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:57:27,003] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_transient_transient_TARGET_AVRO_7080193500955133055_1689926246950 in state PreparingRebalance with old generation 0 (__consumer_offsets-32) (reason: Adding new member _confluent-ksql-default_transient_transient_TARGET_AVRO_7080193500955133055_1689926246950-42bb0896-dfa1-4ac5-8735-71f3b753f050-StreamThread-1-consumer-27ba145b-41ba-4fec-9d37-5873e747766f with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:57:27,003] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_transient_transient_TARGET_AVRO_7080193500955133055_1689926246950 generation 1 (__consumer_offsets-32) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:57:27,007] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_transient_transient_TARGET_AVRO_7080193500955133055_1689926246950-42bb0896-dfa1-4ac5-8735-71f3b753f050-StreamThread-1-consumer-27ba145b-41ba-4fec-9d37-5873e747766f for group _confluent-ksql-default_transient_transient_TARGET_AVRO_7080193500955133055_1689926246950 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:57:39,889] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-ksql-default_transient_transient_SOURCE_376973829634169805_1689926259838 in Empty state. Created a new member id _confluent-ksql-default_transient_transient_SOURCE_376973829634169805_1689926259838-0173ff76-ef88-479c-938b-c44c4527859c-StreamThread-1-consumer-9038f606-28bc-4360-a261-bff2ad1c5f18 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:57:39,890] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_transient_transient_SOURCE_376973829634169805_1689926259838 in state PreparingRebalance with old generation 0 (__consumer_offsets-41) (reason: Adding new member _confluent-ksql-default_transient_transient_SOURCE_376973829634169805_1689926259838-0173ff76-ef88-479c-938b-c44c4527859c-StreamThread-1-consumer-9038f606-28bc-4360-a261-bff2ad1c5f18 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:57:39,891] INFO [GroupCoordinator 0]: Stabilized group _confluent-ksql-default_transient_transient_SOURCE_376973829634169805_1689926259838 generation 1 (__consumer_offsets-41) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:57:39,895] INFO [GroupCoordinator 0]: Assignment received from leader _confluent-ksql-default_transient_transient_SOURCE_376973829634169805_1689926259838-0173ff76-ef88-479c-938b-c44c4527859c-StreamThread-1-consumer-9038f606-28bc-4360-a261-bff2ad1c5f18 for group _confluent-ksql-default_transient_transient_SOURCE_376973829634169805_1689926259838 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:58:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3734 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:58:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 2853 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:58:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3120 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:58:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3904 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:58:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 4230 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:58:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3796 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:58:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 4002 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:58:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3642 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:58:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 4073 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:58:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3522 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:58:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3464 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:58:08,845] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-631490244905089567] Seeking to offset 3107 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2023-07-21 14:58:12,007] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_transient_transient_TARGET_AVRO_7080193500955133055_1689926246950-42bb0896-dfa1-4ac5-8735-71f3b753f050-StreamThread-1-consumer-27ba145b-41ba-4fec-9d37-5873e747766f in group _confluent-ksql-default_transient_transient_TARGET_AVRO_7080193500955133055_1689926246950 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:58:12,007] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_transient_transient_TARGET_AVRO_7080193500955133055_1689926246950 in state PreparingRebalance with old generation 1 (__consumer_offsets-32) (reason: removing member _confluent-ksql-default_transient_transient_TARGET_AVRO_7080193500955133055_1689926246950-42bb0896-dfa1-4ac5-8735-71f3b753f050-StreamThread-1-consumer-27ba145b-41ba-4fec-9d37-5873e747766f on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:58:12,008] INFO [GroupCoordinator 0]: Group _confluent-ksql-default_transient_transient_TARGET_AVRO_7080193500955133055_1689926246950 with generation 2 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:58:12,326] INFO [GroupCoordinator 0]: The following groups were deleted: _confluent-ksql-default_transient_transient_TARGET_AVRO_7080193500955133055_1689926246950. A total of 0 offsets were removed. (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:58:25,143] INFO [GroupCoordinator 0]: Member _confluent-ksql-default_transient_transient_SOURCE_376973829634169805_1689926259838-0173ff76-ef88-479c-938b-c44c4527859c-StreamThread-1-consumer-9038f606-28bc-4360-a261-bff2ad1c5f18 in group _confluent-ksql-default_transient_transient_SOURCE_376973829634169805_1689926259838 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:58:25,143] INFO [GroupCoordinator 0]: Preparing to rebalance group _confluent-ksql-default_transient_transient_SOURCE_376973829634169805_1689926259838 in state PreparingRebalance with old generation 1 (__consumer_offsets-41) (reason: removing member _confluent-ksql-default_transient_transient_SOURCE_376973829634169805_1689926259838-0173ff76-ef88-479c-938b-c44c4527859c-StreamThread-1-consumer-9038f606-28bc-4360-a261-bff2ad1c5f18 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:58:25,144] INFO [GroupCoordinator 0]: Group _confluent-ksql-default_transient_transient_SOURCE_376973829634169805_1689926259838 with generation 2 is now empty (__consumer_offsets-41) (kafka.coordinator.group.GroupCoordinator)
[2023-07-21 14:58:30,343] INFO [GroupCoordinator 0]: The following groups were deleted: _confluent-ksql-default_transient_transient_SOURCE_376973829634169805_1689926259838. A total of 1 offsets were removed. (kafka.coordinator.group.GroupCoordinator)
